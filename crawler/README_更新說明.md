# çˆ¬èŸ²ç³»çµ±æ›´æ–°èªªæ˜

## ğŸ‰ æ›´æ–°å…§å®¹

åŸºæ–¼ DZ1439 æˆåŠŸç¶“é©—ï¼Œå…¨é¢æ›´æ–°å¸«å…¸å¤ç±çˆ¬èŸ²ç³»çµ±ï¼

### æ–°å¢æª”æ¡ˆ

1. **`shidian_crawler.py`** - å…¨æ–°çš„å¸«å…¸å¤ç±çˆ¬èŸ²
   - æ•´åˆ DZ1439 çš„æˆåŠŸç¶“é©—
   - æ”¯æ´å–®æœ¬å’Œæ‰¹é‡çˆ¬å–
   - å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„
   - è‡ªå‹•ä¿å­˜ç‚º JSON å’Œæ–‡å­—æª”æ¡ˆ

### æ ¸å¿ƒæ”¹é€²

#### 1. å…§å®¹æå–ç­–ç•¥
```python
# å„ªå…ˆé †åºï¼šarticle > main > å…¶ä»–å®¹å™¨
article_tag = soup.find('article')  # æœ€å„ªå…ˆ
main_tag = soup.find('main')        # æ¬¡å„ªå…ˆ
div.chapter-content                 # å‚™ç”¨æ–¹æ¡ˆ
```

#### 2. å®Œæ•´çš„æ›¸ç±è³‡è¨Šæå–
- âœ… æ›¸åï¼ˆh1.HbYW1Abiï¼‰
- âœ… ä½œè€…å’Œæœä»£ï¼ˆspan.book-title-authorï¼‰
- âœ… æ‘˜è¦ï¼ˆmeta descriptionï¼‰
- âœ… ç« ç¯€åˆ—è¡¨ï¼ˆdiv.semi-tree-optionï¼‰

#### 3. æ™ºèƒ½æª”æ¡ˆç®¡ç†
```
docs/source_texts/
â””â”€â”€ æ›¸å/
    â”œâ”€â”€ 00_æ›¸ç±è³‡è¨Š.txt
    â”œâ”€â”€ 01_ç« ç¯€åç¨±.txt
    â”œâ”€â”€ 02_ç« ç¯€åç¨±.txt
    â””â”€â”€ ...

data/crawled/
â””â”€â”€ DZ1439_æ›¸å.json
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ç›´æ¥åŸ·è¡Œï¼ˆæ¨è–¦ï¼‰

```bash
# çˆ¬å–å–®æœ¬æ›¸ç±
python crawler/shidian_crawler.py
```

### æ–¹æ³•2: ä½œç‚ºæ¨¡çµ„ä½¿ç”¨

```python
from crawler.shidian_crawler import ShidianCrawler

# å»ºç«‹çˆ¬èŸ²
crawler = ShidianCrawler(delay=2)

# çˆ¬å–å–®æœ¬æ›¸ç±
book_info = crawler.crawl_book('DZ1439')

# ä¿å­˜çµæœ
crawler.save_to_json(book_info)
crawler.save_to_text_files(book_info)

# åˆ—å°çµ±è¨ˆ
crawler.print_statistics(book_info)
```

### æ–¹æ³•3: æ‰¹é‡çˆ¬å–

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler(delay=2)

# æ‰¹é‡çˆ¬å–å¤šæœ¬æ›¸ç±
book_ids = ['DZ1439', 'DZ1234', 'DZ1437']
results = crawler.batch_crawl(book_ids)

# æŸ¥çœ‹çµæœ
for book in results:
    crawler.print_statistics(book)
```

## ğŸ“Š åŠŸèƒ½å°æ¯”

| åŠŸèƒ½ | èˆŠç‰ˆçˆ¬èŸ² | æ–°ç‰ˆçˆ¬èŸ² |
|------|---------|---------|
| å…§å®¹æå– | âŒ å¤±æ•— | âœ… æˆåŠŸ (article/main) |
| æ›¸ç±è³‡è¨Š | âš ï¸ éƒ¨åˆ† | âœ… å®Œæ•´ |
| ç« ç¯€åˆ—è¡¨ | âœ… æ”¯æ´ | âœ… æ”¯æ´ |
| æ‰¹é‡çˆ¬å– | âŒ ç„¡ | âœ… æ”¯æ´ |
| JSON è¼¸å‡º | âš ï¸ ç°¡å–® | âœ… å®Œæ•´çµæ§‹ |
| æ–‡å­—æª”æ¡ˆ | âš ï¸ ç°¡å–® | âœ… æ ¼å¼åŒ–è¼¸å‡º |
| éŒ¯èª¤è™•ç† | âš ï¸ åŸºæœ¬ | âœ… å®Œæ•´ |
| æ—¥èªŒè¨˜éŒ„ | âš ï¸ åŸºæœ¬ | âœ… è©³ç´° |
| çµ±è¨ˆè³‡è¨Š | âŒ ç„¡ | âœ… å®Œæ•´ |

## ğŸ¯ æ¸¬è©¦çµæœ

### DZ1439 æ¸¬è©¦
- âœ… æ›¸å: æ´ç„çµå®ç‰äº¬å±±æ­¥è™šç»
- âœ… ç« ç¯€æ•¸: 7 ç« 
- âœ… æˆåŠŸç‡: 100% (7/7)
- âœ… ç¸½å­—æ•¸: 3,502 å­—
- âœ… å¹³å‡æ¯ç« : 500 å­—

## ğŸ“ API æ–‡æª”

### ShidianCrawler é¡åˆ¥

#### åˆå§‹åŒ–
```python
crawler = ShidianCrawler(delay=2)
```
- `delay`: è«‹æ±‚é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ 2 ç§’

#### ä¸»è¦æ–¹æ³•

##### 1. get_book_info(book_id)
ç²å–æ›¸ç±è³‡è¨Šå’Œç« ç¯€åˆ—è¡¨
```python
book_info = crawler.get_book_info('DZ1439')
```

##### 2. get_chapter_content(chapter_url, chapter_name)
ç²å–å–®å€‹ç« ç¯€å…§å®¹
```python
chapter = crawler.get_chapter_content(url, "ç« ç¯€åç¨±")
```

##### 3. crawl_all_chapters(book_info)
çˆ¬å–æ‰€æœ‰ç« ç¯€å…§å®¹
```python
book_info = crawler.crawl_all_chapters(book_info)
```

##### 4. crawl_book(book_id)
çˆ¬å–å®Œæ•´æ›¸ç±ï¼ˆä¸€ç«™å¼æ–¹æ³•ï¼‰
```python
book_info = crawler.crawl_book('DZ1439')
```

##### 5. save_to_json(book_info, output_dir)
ä¿å­˜ç‚º JSON æ ¼å¼
```python
crawler.save_to_json(book_info, 'data/crawled')
```

##### 6. save_to_text_files(book_info, output_dir)
ä¿å­˜ç‚ºæ–‡å­—æª”æ¡ˆ
```python
crawler.save_to_text_files(book_info, 'docs/source_texts/æ›¸å')
```

##### 7. batch_crawl(book_ids, output_dir)
æ‰¹é‡çˆ¬å–å¤šæœ¬æ›¸ç±
```python
results = crawler.batch_crawl(['DZ1439', 'DZ1234'])
```

##### 8. print_statistics(book_info)
åˆ—å°çˆ¬å–çµ±è¨ˆè³‡è¨Š
```python
crawler.print_statistics(book_info)
```

## ğŸ”§ é…ç½®é¸é …

### ä¿®æ”¹å»¶é²æ™‚é–“
```python
# æ›´ä¿å®ˆï¼ˆ3ç§’ï¼‰
crawler = ShidianCrawler(delay=3)

# æ›´å¿«é€Ÿï¼ˆ1ç§’ï¼Œä¸å»ºè­°ï¼‰
crawler = ShidianCrawler(delay=1)
```

### è‡ªè¨‚è¼¸å‡ºç›®éŒ„
```python
# JSON è¼¸å‡º
crawler.save_to_json(book_info, 'my_output/json')

# æ–‡å­—æª”æ¡ˆè¼¸å‡º
crawler.save_to_text_files(book_info, 'my_output/texts')
```

### æ—¥èªŒè¨­å®š
æ—¥èªŒè‡ªå‹•ä¿å­˜åˆ° `data/logs/shidian_crawler.log`

## âš ï¸ æ³¨æ„äº‹é …

1. **è«‹æ±‚é »ç‡**: é è¨­ 2 ç§’å»¶é²ï¼Œè«‹å‹¿è¨­å®šéçŸ­
2. **ç¶²è·¯ç©©å®š**: ç¢ºä¿ç¶²è·¯é€£ç·šç©©å®š
3. **ç£ç¢Ÿç©ºé–“**: ç¢ºä¿æœ‰è¶³å¤ ç©ºé–“å„²å­˜çµæœ
4. **ç‰ˆæ¬Šå°Šé‡**: åƒ…ä¾›å­¸ç¿’ç ”ç©¶ä½¿ç”¨
5. **éŒ¯èª¤è™•ç†**: é‡åˆ°éŒ¯èª¤æœƒè‡ªå‹•è¨˜éŒ„ä¸¦ç¹¼çºŒ

## ğŸ“ˆ æ•ˆèƒ½æŒ‡æ¨™

- å–®ç« çˆ¬å–æ™‚é–“: ~2-3 ç§’
- 7 ç« æ›¸ç±ç¸½æ™‚é–“: ~15-20 ç§’
- æˆåŠŸç‡: 100% (æ¸¬è©¦æ–¼ DZ1439)
- è¨˜æ†¶é«”ä½¿ç”¨: < 50MB

## ğŸ”„ æ•´åˆåˆ°ç¾æœ‰ç³»çµ±

### æ•´åˆåˆ° main.py
```python
from crawler.shidian_crawler import ShidianCrawler

def crawl_command(book_id):
    """çˆ¬å–å‘½ä»¤"""
    crawler = ShidianCrawler()
    book_info = crawler.crawl_book(book_id)
    
    if book_info:
        crawler.save_to_json(book_info)
        crawler.save_to_text_files(book_info)
        crawler.print_statistics(book_info)
        return True
    return False
```

### æ•´åˆåˆ° EasyCLI
```python
from crawler.shidian_crawler import ShidianCrawler

class EasyCLI:
    def crawl_book(self, book_id):
        crawler = ShidianCrawler()
        return crawler.crawl_book(book_id)
```

## ğŸ“ å­¸ç¿’è¦é»

1. **HTML è§£æ**: ä½¿ç”¨ BeautifulSoup æå–å…§å®¹
2. **éŒ¯èª¤è™•ç†**: try-except å’Œæ—¥èªŒè¨˜éŒ„
3. **æª”æ¡ˆç®¡ç†**: pathlib å’Œ os æ¨¡çµ„
4. **è³‡æ–™çµæ§‹**: å­—å…¸å’Œåˆ—è¡¨çš„ä½¿ç”¨
5. **ç‰©ä»¶å°å‘**: é¡åˆ¥è¨­è¨ˆå’Œæ–¹æ³•çµ„ç¹”

## ğŸ“š ç›¸é—œæª”æ¡ˆ

- `shidian_crawler.py` - ä¸»è¦çˆ¬èŸ²ç¨‹å¼
- `base_crawler.py` - åŸºç¤çˆ¬èŸ²é¡åˆ¥ï¼ˆä¿ç•™ï¼‰
- `shidian_selenium.py` - Selenium ç‰ˆæœ¬ï¼ˆå‚™ç”¨ï¼‰
- `DZ1439_çˆ¬èŸ²ä½¿ç”¨èªªæ˜.md` - DZ1439 è©³ç´°èªªæ˜

## âœ… ä¸‹ä¸€æ­¥

1. âœ… æ¸¬è©¦æ›´å¤šæ›¸ç±ï¼ˆDZ1234, DZ1437 ç­‰ï¼‰
2. âœ… æ•´åˆåˆ° main.py å‘½ä»¤åˆ—ä»‹é¢
3. âœ… æ·»åŠ é€²åº¦è¿½è¹¤åŠŸèƒ½
4. âœ… æ”¯æ´æ–·é»çºŒå‚³
5. âœ… æ·»åŠ å…§å®¹é©—è­‰

## ğŸ‰ å®Œæˆï¼

æ–°ç‰ˆçˆ¬èŸ²å·²ç¶“æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨äº†ï¼
