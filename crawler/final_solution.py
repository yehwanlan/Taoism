#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€çµ‚è§£æ±ºæ–¹æ¡ˆ - åå…¸å¤ç±çˆ¬èŸ²

å­¸ç¿’é‡é»ï¼š
1. å¦‚ä½•å¾è¤‡é›œçš„APIå›æ‡‰ä¸­æå–æœ‰ç”¨è³‡è¨Š
2. å¦‚ä½•è™•ç†åµŒå¥—çš„JSONæ•¸æ“šçµæ§‹
3. å¦‚ä½•å»ºç«‹å®Œæ•´çš„çˆ¬èŸ²è§£æ±ºæ–¹æ¡ˆ
4. å¦‚ä½•æ•´åˆå¤šç¨®çˆ¬èŸ²æŠ€è¡“
"""

import json
import re
from urllib.parse import urljoin
from base_crawler import BaseCrawler

class FinalSolution(BaseCrawler):
    """æœ€çµ‚è§£æ±ºæ–¹æ¡ˆçˆ¬èŸ²"""
    
    def __init__(self):
        super().__init__(delay_range=(2, 4))
        self.base_url = "https://www.shidianguji.com"
        
    def extract_book_info_from_api(self, url):
        """å¾APIä¸­æå–æ›¸ç±è³‡è¨Š"""
        print(f"ğŸ“š å¾APIæå–æ›¸ç±è³‡è¨Š: {url}")
        
        response = self.make_request(url)
        if not response:
            return None
            
        # å˜—è©¦å¾å›æ‡‰ä¸­æå–JSONæ•¸æ“š
        text = response.text
        
        # å°‹æ‰¾ window._ROUTER_DATA æˆ–é¡ä¼¼çš„æ•¸æ“š
        patterns = [
            r'window\._ROUTER_DATA\s*=\s*({.*?});',
            r'window\._SSR_DATA\s*=\s*({.*?});',
            r'"bookInfo":\s*({.*?})',
            r'"chapterInfo":\s*({.*?})',
        ]
        
        for pattern in patterns:
            matches = re.search(pattern, text, re.DOTALL)
            if matches:
                try:
                    data = json.loads(matches.group(1))
                    print(f"âœ… æ‰¾åˆ°æ•¸æ“šçµæ§‹: {pattern}")
                    return data
                except json.JSONDecodeError:
                    continue
                    
        return None
        
    def find_content_api_from_book_info(self, book_info, chapter_id):
        """å¾æ›¸ç±è³‡è¨Šä¸­æ‰¾åˆ°å…§å®¹API"""
        print(f"ğŸ” å¾æ›¸ç±è³‡è¨Šä¸­å°‹æ‰¾å…§å®¹API...")
        
        if not book_info:
            return None
            
        # éæ­¸æœå°‹ç« ç¯€è³‡è¨Š
        def find_chapter_info(obj, target_id):
            if isinstance(obj, dict):
                if obj.get('chapterId') == target_id:
                    return obj
                for value in obj.values():
                    result = find_chapter_info(value, target_id)
                    if result:
                        return result
            elif isinstance(obj, list):
                for item in obj:
                    result = find_chapter_info(item, target_id)
                    if result:
                        return result
            return None
            
        chapter_info = find_chapter_info(book_info, chapter_id)
        
        if chapter_info:
            print(f"âœ… æ‰¾åˆ°ç« ç¯€è³‡è¨Š: {chapter_info.get('chapterName', 'Unknown')}")
            
            # å˜—è©¦æ§‹å»ºå…§å®¹API URL
            volume_id = chapter_info.get('volumeId')
            if volume_id:
                content_api = f"/api/ancientlib/volume/{volume_id}/content"
                return content_api
                
        return None
        
    def get_chapter_content(self, content_api_url):
        """ç²å–ç« ç¯€å…§å®¹"""
        print(f"ğŸ“– ç²å–ç« ç¯€å…§å®¹: {content_api_url}")
        
        full_url = urljoin(self.base_url, content_api_url)
        response = self.make_request(full_url)
        
        if not response:
            return None
            
        try:
            data = response.json()
            
            # å¾JSONä¸­æå–æ–‡æœ¬å…§å®¹
            content_parts = []
            
            def extract_text_recursive(obj):
                if isinstance(obj, dict):
                    # å°‹æ‰¾åŒ…å«æ–‡æœ¬çš„å­—æ®µ
                    if 'content' in obj and isinstance(obj['content'], str):
                        text = obj['content'].strip()
                        if len(text) > 10 and re.search(r'[\u4e00-\u9fff]', text):
                            content_parts.append(text)
                    
                    for value in obj.values():
                        extract_text_recursive(value)
                        
                elif isinstance(obj, list):
                    for item in obj:
                        extract_text_recursive(item)
                        
            extract_text_recursive(data)
            
            if content_parts:
                final_content = '\n\n'.join(content_parts)
                print(f"âœ… æå–åˆ°å…§å®¹ï¼Œç¸½é•·åº¦: {len(final_content)} å­—ç¬¦")
                return final_content
            else:
                print("âŒ æœªèƒ½å¾JSONä¸­æå–æ–‡æœ¬å…§å®¹")
                # å„²å­˜åŸå§‹æ•¸æ“šä»¥ä¾›èª¿è©¦
                with open("content_debug.json", "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print("ğŸ’¾ åŸå§‹æ•¸æ“šå·²å„²å­˜ç‚º content_debug.json")
                return None
                
        except json.JSONDecodeError:
            print("âŒ APIå›æ‡‰ä¸æ˜¯æœ‰æ•ˆçš„JSON")
            return None
            
    def crawl_shidian_final(self, url, title=None):
        """æœ€çµ‚çˆ¬å–æ–¹æ¡ˆ"""
        print(f"ğŸ¯ æœ€çµ‚çˆ¬å–æ–¹æ¡ˆ")
        print(f"ç¶²å€: {url}")
        print("=" * 60)
        
        # 1. å¾URLæå–ID
        import re
        book_match = re.search(r'/book/([^/]+)', url)
        chapter_match = re.search(r'/chapter/([^/?]+)', url)
        
        if not book_match or not chapter_match:
            print("âŒ ç„¡æ³•å¾URLä¸­æå–æ›¸ç±æˆ–ç« ç¯€ID")
            return False
            
        book_id = book_match.group(1)
        chapter_id = chapter_match.group(1)
        
        print(f"æ›¸ç±ID: {book_id}")
        print(f"ç« ç¯€ID: {chapter_id}")
        
        # 2. ç²å–æ›¸ç±è³‡è¨Š
        book_api = f"/api/book/{book_id}/chapter/{chapter_id}"
        book_info = self.extract_book_info_from_api(urljoin(self.base_url, book_api))
        
        if not book_info:
            print("âŒ ç„¡æ³•ç²å–æ›¸ç±è³‡è¨Š")
            return False
            
        # 3. å°‹æ‰¾å…§å®¹API
        content_api = self.find_content_api_from_book_info(book_info, chapter_id)
        
        if not content_api:
            print("âŒ ç„¡æ³•æ‰¾åˆ°å…§å®¹API")
            return False
            
        # 4. ç²å–å¯¦éš›å…§å®¹
        content = self.get_chapter_content(content_api)
        
        if not content:
            print("âŒ ç„¡æ³•ç²å–ç« ç¯€å…§å®¹")
            return False
            
        # 5. æ¸…ç†å’Œå„²å­˜å…§å®¹
        cleaned_content = self.clean_content(content)
        
        if not title:
            title = f"{book_id}_{chapter_id}_æœ€çµ‚ç‰ˆæœ¬"
            
        filename = f"{title}.txt"
        self.save_text(cleaned_content, filename, "../docs/source_texts")
        
        print(f"âœ… æœ€çµ‚çˆ¬å–æˆåŠŸ: {title}")
        print(f"å…§å®¹é•·åº¦: {len(cleaned_content)} å­—ç¬¦")
        
        return True
        
    def clean_content(self, content):
        """æ¸…ç†å…§å®¹"""
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        # ç§»é™¤HTMLæ¨™ç±¤
        content = re.sub(r'<[^>]+>', '', content)
        # æ•´ç†æ®µè½
        content = re.sub(r'\n\s*\n', '\n\n', content)
        return content.strip()

# ä½¿ç”¨ç¤ºä¾‹
def test_final_solution():
    """æ¸¬è©¦æœ€çµ‚è§£æ±ºæ–¹æ¡ˆ"""
    crawler = FinalSolution()
    
    url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1?page_from=home_page&version=19"
    
    success = crawler.crawl_shidian_final(url, "æŠ±æœ´å­_å…§ç¯‡_æœ€çµ‚ç‰ˆæœ¬")
    
    if success:
        print("\nğŸ‰ æœ€çµ‚çˆ¬å–å®Œæˆï¼")
        print("ğŸ”§ ç¾åœ¨æ‚¨å·²ç¶“æŒæ¡äº†å®Œæ•´çš„çˆ¬èŸ²æŠ€è¡“æ£§ï¼š")
        print("   1. åŸºç¤HTTPçˆ¬èŸ²")
        print("   2. APIé€†å‘å·¥ç¨‹")
        print("   3. JSONæ•¸æ“šè§£æ")
        print("   4. å‹•æ…‹å…§å®¹è™•ç†")
    else:
        print("\nâŒ æœ€çµ‚çˆ¬å–å¤±æ•—")
        print("ğŸ’¡ æª¢æŸ¥èª¿è©¦æ–‡ä»¶ä»¥äº†è§£æ›´å¤šè³‡è¨Š")

if __name__ == "__main__":
    test_final_solution()