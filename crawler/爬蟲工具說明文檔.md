# 道教經典翻譯專案 - 爬蟲工具說明文檔

## 📁 目錄結構概覽

```
crawler/
├── 📋 說明文檔
│   ├── 爬蟲工具說明文檔.md      # 本文檔
│   ├── README.md                # 基礎說明
│   └── practical_guide.md       # 實用指南
│
├── 🔧 核心爬蟲引擎
│   ├── base_crawler.py          # 爬蟲基礎類別
│   ├── taoism_crawler.py        # 道教經典專用爬蟲
│   └── api_crawler.py           # API逆向工程爬蟲
│
├── 🌐 網站專用爬蟲
│   ├── shidian_simple.py        # 十典古籍網簡化版（推薦）
│   ├── shidian_crawler.py       # 十典古籍網完整版
│   └── baopuzi_crawler.py       # 抱朴子專用爬蟲
│
├── 🔍 分析工具
│   ├── page_analyzer.py         # 網頁結構分析器
│   ├── url_finder.py            # URL搜尋工具
│   └── smart_crawler.py         # 智能內容提取器
│
├── 🚀 進階工具
│   ├── selenium_crawler.py      # 動態網頁爬蟲
│   ├── final_solution.py        # 最終解決方案
│   └── run_crawler.py           # 命令列控制器
│
├── 🧪 測試和示例
│   ├── demo.py                  # 基礎示例
│   ├── improved_demo.py         # 改進示例
│   └── debug_*.txt              # 調試檔案
│
└── ⚙️ 配置和依賴
    ├── requirements.txt         # Python依賴套件
    └── crawler.log             # 運行日誌
```

## 🎯 核心工具詳解

### 1. 基礎爬蟲引擎

#### `base_crawler.py` - 爬蟲基礎類別
**功能**：提供所有爬蟲的基礎功能
**特色**：
- ✅ HTTP請求處理和重試機制
- ✅ 中文編碼自動處理
- ✅ 隨機User-Agent和延遲
- ✅ 完整的日誌記錄系統
- ✅ 錯誤處理和異常捕獲

**使用場景**：作為其他爬蟲的父類別

#### `taoism_crawler.py` - 道教經典專用爬蟲
**功能**：專門處理道教經典文本
**特色**：
- ✅ 古文文本清理和格式化
- ✅ 道教術語識別和保護
- ✅ 結構化資料儲存
- ✅ 批量處理多個經典

**使用場景**：處理傳統道教網站的經典文本

#### `api_crawler.py` - API逆向工程爬蟲
**功能**：直接調用網站API獲取數據
**特色**：
- ✅ 自動分析URL結構
- ✅ API端點自動發現
- ✅ JSON數據解析
- ✅ 繞過前端限制

**使用場景**：處理現代SPA網站的動態內容

### 2. 網站專用爬蟲

#### `shidian_simple.py` - 十典古籍網簡化版 ⭐ **推薦**
**功能**：專門爬取十典古籍網內容的最佳解決方案
**特色**：
- ✅ 自動識別書籍和章節ID
- ✅ 多API端點自動嘗試
- ✅ HTML內容智能解析
- ✅ 中文內容品質驗證
- ✅ 簡單易用的介面

**使用方法**：
```python
from shidian_simple import ShidianSimple

crawler = ShidianSimple()
success = crawler.crawl(url, "輸出檔名")
```

#### `shidian_crawler.py` - 十典古籍網完整版
**功能**：功能更完整的十典古籍網爬蟲
**特色**：
- ✅ 完整的錯誤處理
- ✅ 詳細的進度報告
- ✅ 多種內容提取策略
- ✅ 可配置的參數設定

#### `baopuzi_crawler.py` - 抱朴子專用爬蟲
**功能**：專門處理抱朴子各章節
**特色**：
- ✅ 章節自動編號
- ✅ 標題自動提取
- ✅ 檔案名稱自動清理
- ✅ 專案結構自動建立

### 3. 分析工具

#### `page_analyzer.py` - 網頁結構分析器
**功能**：深度分析網頁結構，找出內容提取策略
**特色**：
- ✅ DOM結構詳細分析
- ✅ JavaScript載入檢測
- ✅ CSS類別統計分析
- ✅ 內容容器自動識別

**使用場景**：分析新網站時的第一步工具

#### `url_finder.py` - URL搜尋工具
**功能**：自動搜尋和驗證相關網址
**特色**：
- ✅ 關鍵字自動搜尋
- ✅ 網址有效性驗證
- ✅ 內容品質評估
- ✅ 批量URL處理

#### `smart_crawler.py` - 智能內容提取器
**功能**：使用多種策略智能提取內容
**特色**：
- ✅ 多策略內容提取
- ✅ 古文特徵自動識別
- ✅ 內容品質自動評估
- ✅ 原始HTML保存供調試

### 4. 進階工具

#### `selenium_crawler.py` - 動態網頁爬蟲
**功能**：處理JavaScript渲染的動態內容
**特色**：
- ✅ 真實瀏覽器模擬
- ✅ 動態內容等待機制
- ✅ 多種內容選擇器
- ✅ 無頭模式支援

**注意**：需要安裝Chrome瀏覽器和ChromeDriver

#### `final_solution.py` - 最終解決方案
**功能**：整合多種技術的綜合解決方案
**特色**：
- ✅ 多種爬蟲技術整合
- ✅ 自動選擇最佳策略
- ✅ 完整的錯誤恢復
- ✅ 詳細的調試資訊

#### `run_crawler.py` - 命令列控制器
**功能**：提供命令列介面控制所有爬蟲
**特色**：
- ✅ 統一的命令列介面
- ✅ 配置檔案支援
- ✅ 批量處理模式
- ✅ 進度追蹤和報告

## 🚀 快速開始指南

### 1. 環境設定
```bash
# 安裝依賴
pip install -r requirements.txt

# 檢查安裝
python -c "import requests, bs4; print('✅ 基礎套件安裝成功')"
```

### 2. 基本使用

#### 爬取單一頁面（推薦方法）
```python
from shidian_simple import ShidianSimple

crawler = ShidianSimple()
url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1"
success = crawler.crawl(url, "抱朴子_第一章")

if success:
    print("✅ 爬取成功！")
```

#### 使用命令列工具
```bash
# 驗證網址
python run_crawler.py --mode validate --url "網址"

# 爬取內容
python run_crawler.py --mode crawl --config crawler_config.json
```

#### 分析新網站
```python
from page_analyzer import PageAnalyzer

analyzer = PageAnalyzer()
result = analyzer.deep_analyze("新網站URL")
```

### 3. 進階使用

#### 批量處理
```python
from baopuzi_crawler import BaopuziCrawler

crawler = BaopuziCrawler()
chapters = [
    {'number': 1, 'title': '章節1', 'url': 'URL1'},
    {'number': 2, 'title': '章節2', 'url': 'URL2'},
]

for chapter in chapters:
    crawler.crawl_chapter(chapter['url'], chapter['number'])
```

#### 動態內容處理
```python
from selenium_crawler import SeleniumCrawler

crawler = SeleniumCrawler(headless=True)
success = crawler.crawl_dynamic_page(url, "標題")
crawler.close()
```

## 🛠️ 工具選擇指南

### 根據網站類型選擇

| 網站類型 | 推薦工具 | 備選方案 |
|---------|---------|---------|
| 十典古籍網 | `shidian_simple.py` | `api_crawler.py` |
| 傳統靜態網站 | `taoism_crawler.py` | `base_crawler.py` |
| 現代SPA網站 | `api_crawler.py` | `selenium_crawler.py` |
| 未知網站 | `page_analyzer.py` | `smart_crawler.py` |

### 根據需求選擇

| 需求 | 推薦工具 | 說明 |
|------|---------|------|
| 快速爬取 | `shidian_simple.py` | 最簡單直接 |
| 批量處理 | `baopuzi_crawler.py` | 專門設計 |
| 調試分析 | `page_analyzer.py` | 詳細分析 |
| 動態內容 | `selenium_crawler.py` | 處理JS渲染 |
| 綜合解決 | `final_solution.py` | 多技術整合 |

## 📊 效能和限制

### 效能對比

| 工具 | 速度 | 成功率 | 資源消耗 | 適用範圍 |
|------|------|--------|----------|----------|
| `shidian_simple.py` | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 十典古籍網 |
| `api_crawler.py` | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 現代網站 |
| `selenium_crawler.py` | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | 動態網站 |
| `taoism_crawler.py` | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | 傳統網站 |

### 使用限制

- **網路限制**：需要穩定的網路連接
- **反爬蟲**：部分網站可能有反爬蟲機制
- **法律合規**：請遵守網站使用條款
- **資源消耗**：Selenium工具消耗較多系統資源

## 🔧 故障排除

### 常見問題

#### 1. 安裝問題
```bash
# 如果pip安裝失敗
pip install --upgrade pip
pip install -r requirements.txt --no-cache-dir
```

#### 2. 編碼問題
```python
# 確保使用UTF-8編碼
with open(file_path, 'w', encoding='utf-8') as f:
    f.write(content)
```

#### 3. 網路問題
```python
# 增加重試次數和延遲
crawler = BaseCrawler(delay_range=(3, 6))
response = crawler.make_request(url, max_retries=5)
```

#### 4. Selenium問題
```bash
# 安裝ChromeDriver
# 下載：https://chromedriver.chromium.org/
# 或使用webdriver-manager
pip install webdriver-manager
```

### 調試技巧

1. **查看日誌**：檢查 `crawler.log` 檔案
2. **保存調試檔案**：使用 `debug_*.txt` 檔案分析
3. **逐步測試**：先用 `page_analyzer.py` 分析網站
4. **網路監控**：使用瀏覽器開發者工具監控請求

## 📈 進階開發

### 擴展新網站支援

1. **分析網站結構**
```python
from page_analyzer import PageAnalyzer
analyzer = PageAnalyzer()
analyzer.deep_analyze("新網站URL")
```

2. **建立專用爬蟲**
```python
from base_crawler import BaseCrawler

class NewSiteCrawler(BaseCrawler):
    def __init__(self):
        super().__init__()
        # 自定義初始化
    
    def extract_content(self, url):
        # 實作內容提取邏輯
        pass
```

3. **測試和優化**
```python
# 測試不同的選擇器
selectors = ['.content', '#main', 'article']
for selector in selectors:
    elements = soup.select(selector)
    if elements:
        print(f"找到內容: {selector}")
```

### 整合AI翻譯

```python
def integrate_ai_translation(text):
    # 可以整合：
    # 1. OpenAI API
    # 2. Google Translate API
    # 3. 本地AI模型
    # 4. 其他翻譯服務
    
    translated = ai_translate_service(text)
    return translated
```

## 📚 學習資源

### 相關技術文檔
- [Requests文檔](https://docs.python-requests.org/)
- [BeautifulSoup文檔](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium文檔](https://selenium-python.readthedocs.io/)

### 最佳實踐
1. **遵守robots.txt**
2. **設定適當延遲**
3. **處理異常情況**
4. **記錄詳細日誌**
5. **測試不同場景**

---

## 🎉 總結

這個爬蟲工具集提供了從基礎到進階的完整解決方案，特別針對古籍網站進行了優化。無論您是初學者還是進階用戶，都能找到適合的工具。

**推薦使用順序**：
1. 🥇 `shidian_simple.py` - 十典古籍網的最佳選擇
2. 🥈 `page_analyzer.py` - 分析新網站的第一步
3. 🥉 `api_crawler.py` - 處理現代網站的通用方案

**技術支援**：
- 查看 `practical_guide.md` 獲取實用指南
- 檢查 `crawler.log` 了解運行狀況
- 使用調試檔案分析問題

祝您爬蟲愉快！ 🕷️✨