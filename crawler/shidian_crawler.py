#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå…¸å¤ç±ç¶²å°ˆç”¨çˆ¬èŸ²

å­¸ç¿’é‡é»ï¼š
1. å¦‚ä½•åˆ†æç‰¹å®šç¶²ç«™çš„çµæ§‹
2. å¦‚ä½•è™•ç†å‹•æ…‹è¼‰å…¥çš„å…§å®¹
3. å¦‚ä½•æå–ç‰¹å®šæ ¼å¼çš„æ–‡æœ¬
4. å¦‚ä½•è™•ç†è¤‡é›œçš„ç¶²é çµæ§‹
"""

import re
import time
from urllib.parse import urljoin, urlparse
from base_crawler import BaseCrawler

class ShidianCrawler(BaseCrawler):
    """åå…¸å¤ç±ç¶²å°ˆç”¨çˆ¬èŸ²"""
    
    def __init__(self):
        super().__init__(delay_range=(3, 5))  # è¼ƒé•·å»¶é²é¿å…è¢«å°
        self.base_url = "https://www.shidianguji.com"
        
    def analyze_page_structure(self, url):
        """
        åˆ†æé é¢çµæ§‹
        
        å­¸ç¿’é‡é»ï¼šå¦‚ä½•åˆ†æç¶²é çš„DOMçµæ§‹
        """
        print(f"ğŸ” åˆ†æé é¢çµæ§‹: {url}")
        
        response = self.make_request(url)
        if not response:
            return None
            
        soup = self.parse_html(response.text)
        
        # åˆ†æé é¢çš„ä¸»è¦å…ƒç´ 
        analysis = {
            'title': None,
            'content_containers': [],
            'text_elements': [],
            'scripts': [],
            'meta_info': {}
        }
        
        # æå–æ¨™é¡Œ
        title_selectors = ['title', 'h1', 'h2', '.title', '#title']
        for selector in title_selectors:
            elements = soup.select(selector)
            if elements:
                analysis['title'] = elements[0].get_text().strip()
                break
                
        # å°‹æ‰¾å¯èƒ½çš„å…§å®¹å®¹å™¨
        content_selectors = [
            '.content', '.article', '.text', '.chapter',
            '#content', '#article', '#text', '#chapter',
            '[class*="content"]', '[class*="text"]', '[class*="chapter"]'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements:
                    text = elem.get_text().strip()
                    if len(text) > 50:  # åªè¨˜éŒ„æœ‰æ„ç¾©çš„å…§å®¹
                        analysis['content_containers'].append({
                            'selector': selector,
                            'text_length': len(text),
                            'text_preview': text[:100] + '...' if len(text) > 100 else text
                        })
        
        # æª¢æŸ¥æ˜¯å¦æœ‰JavaScriptå‹•æ…‹è¼‰å…¥
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and ('ajax' in script.string.lower() or 'fetch' in script.string.lower()):
                analysis['scripts'].append('å¯èƒ½æœ‰å‹•æ…‹è¼‰å…¥')
                
        return analysis
        
    def extract_shidian_content(self, url):
        """
        æå–åå…¸å¤ç±ç¶²çš„å…§å®¹
        
        å­¸ç¿’é‡é»ï¼šé‡å°ç‰¹å®šç¶²ç«™çš„å…§å®¹æå–ç­–ç•¥
        """
        print(f"ğŸ“– é–‹å§‹æå–å…§å®¹: {url}")
        
        response = self.make_request(url)
        if not response:
            return None
            
        soup = self.parse_html(response.text)
        
        # åå…¸å¤ç±ç¶²çš„ç‰¹å®šé¸æ“‡å™¨ï¼ˆéœ€è¦æ ¹æ“šå¯¦éš›é é¢èª¿æ•´ï¼‰
        content_selectors = [
            '.chapter-content',
            '.book-content', 
            '.text-content',
            '#chapter-text',
            '.content-text',
            'article',
            '.main-content'
        ]
        
        content = ""
        
        # å˜—è©¦ä¸åŒçš„é¸æ“‡å™¨
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements:
                    text = elem.get_text()
                    if len(text) > content.__len__():
                        content = text
                        print(f"âœ… æ‰¾åˆ°å…§å®¹ (é¸æ“‡å™¨: {selector}, é•·åº¦: {len(text)})")
                        
        # å¦‚æœæ²’æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå˜—è©¦æå–æ‰€æœ‰æ®µè½
        if len(content) < 100:
            paragraphs = soup.find_all(['p', 'div'])
            all_text = []
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 20 and not any(skip in text.lower() for skip in ['copyright', 'ç‰ˆæ¬Š', 'å°èˆª', 'nav']):
                    all_text.append(text)
            content = '\n'.join(all_text)
            print(f"ğŸ“ æå–æ®µè½å…§å®¹ï¼Œç¸½é•·åº¦: {len(content)}")
            
        return self.clean_text(content) if content else None
        
    def get_book_info(self, url):
        """
        æå–æ›¸ç±è³‡è¨Š
        
        å­¸ç¿’é‡é»ï¼šå¦‚ä½•å¾URLå’Œé é¢æå–çµæ§‹åŒ–è³‡è¨Š
        """
        # å¾URLåˆ†ææ›¸ç±è³‡è¨Š
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        
        book_info = {
            'book_id': None,
            'chapter_id': None,
            'title': None,
            'chapter_title': None
        }
        
        # åˆ†æURLçµæ§‹
        for i, part in enumerate(path_parts):
            if part.startswith('SBCK'):
                book_info['book_id'] = part
            elif 'chapter' in parsed_url.path and i < len(path_parts) - 1:
                book_info['chapter_id'] = path_parts[i + 1]
                
        return book_info
        
    def crawl_shidian_page(self, url, custom_title=None):
        """
        çˆ¬å–åå…¸å¤ç±ç¶²çš„å–®ä¸€é é¢
        
        Args:
            url: ç›®æ¨™ç¶²å€
            custom_title: è‡ªå®šç¾©æ¨™é¡Œ
            
        Returns:
            æ˜¯å¦æˆåŠŸçˆ¬å–
        """
        print(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å–åå…¸å¤ç±é é¢")
        print(f"ç¶²å€: {url}")
        print("-" * 50)
        
        # å…ˆåˆ†æé é¢çµæ§‹
        analysis = self.analyze_page_structure(url)
        if analysis:
            print("ğŸ“Š é é¢åˆ†æçµæœ:")
            print(f"æ¨™é¡Œ: {analysis['title']}")
            print(f"æ‰¾åˆ° {len(analysis['content_containers'])} å€‹å…§å®¹å®¹å™¨")
            for container in analysis['content_containers'][:3]:  # åªé¡¯ç¤ºå‰3å€‹
                print(f"  - {container['selector']}: {container['text_length']} å­—ç¬¦")
                print(f"    é è¦½: {container['text_preview']}")
            print()
        
        # æå–å…§å®¹
        content = self.extract_shidian_content(url)
        
        if not content or len(content) < 100:
            print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆå…§å®¹")
            return False
            
        # ç¢ºå®šæ¨™é¡Œ
        if custom_title:
            title = custom_title
        elif analysis and analysis['title']:
            title = analysis['title']
        else:
            book_info = self.get_book_info(url)
            title = f"{book_info['book_id']}_{book_info['chapter_id']}" if book_info['book_id'] else "åå…¸å¤ç±_æœªçŸ¥"
            
        # å„²å­˜å…§å®¹
        filename = f"{title}.txt"
        self.save_text(content, filename, "../docs/source_texts")
        
        print(f"âœ… æˆåŠŸçˆ¬å–: {title}")
        print(f"å…§å®¹é•·åº¦: {len(content)} å­—ç¬¦")
        print(f"å·²å„²å­˜ç‚º: {filename}")
        
        return True

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    crawler = ShidianCrawler()
    
    # æ¸¬è©¦ç¶²å€
    test_url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1?page_from=home_page&version=19"
    
    success = crawler.crawl_shidian_page(test_url, "é“å¾·ç¶“_ç¬¬ä¸€ç« _åå…¸å¤ç±ç‰ˆ")
    
    if success:
        print("\nğŸ‰ çˆ¬å–æˆåŠŸï¼")
    else:
        print("\nâŒ çˆ¬å–å¤±æ•—ï¼Œå¯èƒ½éœ€è¦èª¿æ•´ç­–ç•¥")