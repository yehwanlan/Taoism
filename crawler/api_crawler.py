#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIçˆ¬èŸ² - é‡å°å‹•æ…‹è¼‰å…¥å…§å®¹çš„è§£æ±ºæ–¹æ¡ˆ

å­¸ç¿’é‡é»ï¼š
1. å¦‚ä½•åˆ†æç¶²ç«™çš„APIè«‹æ±‚
2. å¦‚ä½•ç›´æ¥èª¿ç”¨APIç²å–æ•¸æ“š
3. å¦‚ä½•è™•ç†JSONæ ¼å¼çš„å›æ‡‰
4. å¦‚ä½•ç¹éå‰ç«¯é™åˆ¶ç›´æ¥ç²å–å…§å®¹
"""

import json
import re
from urllib.parse import urljoin, urlparse, parse_qs
from base_crawler import BaseCrawler

class APICrawler(BaseCrawler):
    """APIçˆ¬èŸ²"""
    
    def __init__(self):
        super().__init__(delay_range=(2, 4))
        self.base_url = "https://www.shidianguji.com"
        
    def analyze_url_structure(self, url):
        """åˆ†æURLçµæ§‹ä»¥æ¨æ¸¬APIç«¯é»"""
        print(f"ğŸ” åˆ†æURLçµæ§‹: {url}")
        
        parsed = urlparse(url)
        path_parts = parsed.path.split('/')
        query_params = parse_qs(parsed.query)
        
        analysis = {
            'domain': parsed.netloc,
            'path_parts': path_parts,
            'query_params': query_params,
            'book_id': None,
            'chapter_id': None
        }
        
        # å¾URLä¸­æå–æ›¸ç±å’Œç« ç¯€ID
        for part in path_parts:
            if part.startswith('SBCK'):
                analysis['book_id'] = part
            elif len(part) > 10 and '_' in part:
                analysis['chapter_id'] = part
                
        print(f"æ›¸ç±ID: {analysis['book_id']}")
        print(f"ç« ç¯€ID: {analysis['chapter_id']}")
        
        return analysis
        
    def try_api_endpoints(self, book_id, chapter_id):
        """å˜—è©¦å¸¸è¦‹çš„APIç«¯é»"""
        print(f"ğŸ”Œ å˜—è©¦APIç«¯é»...")
        
        # å¸¸è¦‹çš„APIç«¯é»æ¨¡å¼
        api_patterns = [
            f"/api/ancientlib/book/{book_id}/chapter/{chapter_id}",
            f"/api/book/{book_id}/chapter/{chapter_id}",
            f"/api/chapter/{chapter_id}",
            f"/api/content/{book_id}/{chapter_id}",
            f"/api/ancientlib/chapter/{chapter_id}",
            f"/api/v1/book/{book_id}/chapter/{chapter_id}",
        ]
        
        successful_responses = []
        
        for pattern in api_patterns:
            api_url = urljoin(self.base_url, pattern)
            print(f"å˜—è©¦: {api_url}")
            
            response = self.make_request(api_url)
            if response and response.status_code == 200:
                try:
                    data = response.json()
                    if data and isinstance(data, dict):
                        print(f"âœ… æˆåŠŸ: {api_url}")
                        successful_responses.append({
                            'url': api_url,
                            'data': data
                        })
                except:
                    # å¯èƒ½æ˜¯HTMLæˆ–å…¶ä»–æ ¼å¼
                    if len(response.text) > 100:
                        print(f"âœ… æˆåŠŸ (éJSON): {api_url}")
                        successful_responses.append({
                            'url': api_url,
                            'data': response.text
                        })
            else:
                print(f"âŒ å¤±æ•—: {api_url}")
                
        return successful_responses
        
    def extract_content_from_api_response(self, api_data):
        """å¾APIå›æ‡‰ä¸­æå–å…§å®¹"""
        print("ğŸ“– å¾APIå›æ‡‰ä¸­æå–å…§å®¹...")
        
        if isinstance(api_data, dict):
            # å¸¸è¦‹çš„å…§å®¹å­—æ®µå
            content_fields = [
                'content', 'text', 'body', 'data', 
                'chapter_content', 'book_content',
                'original_text', 'content_text'
            ]
            
            for field in content_fields:
                if field in api_data:
                    content = api_data[field]
                    if isinstance(content, str) and len(content) > 50:
                        print(f"âœ… æ‰¾åˆ°å…§å®¹å­—æ®µ: {field}")
                        return content
                        
            # å¦‚æœæ²’æœ‰ç›´æ¥çš„å…§å®¹å­—æ®µï¼Œå˜—è©¦éæ­¸æœå°‹
            def find_content_recursive(obj, depth=0):
                if depth > 3:  # é¿å…ç„¡é™éæ­¸
                    return None
                    
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        if isinstance(value, str) and len(value) > 100:
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
                            if re.search(r'[\u4e00-\u9fff]', value):
                                return value
                        elif isinstance(value, (dict, list)):
                            result = find_content_recursive(value, depth + 1)
                            if result:
                                return result
                elif isinstance(obj, list):
                    for item in obj:
                        result = find_content_recursive(item, depth + 1)
                        if result:
                            return result
                return None
                
            recursive_content = find_content_recursive(api_data)
            if recursive_content:
                print("âœ… é€šééæ­¸æœå°‹æ‰¾åˆ°å…§å®¹")
                return recursive_content
                
        elif isinstance(api_data, str):
            # ç›´æ¥æ˜¯å­—ç¬¦ä¸²å…§å®¹
            if len(api_data) > 50 and re.search(r'[\u4e00-\u9fff]', api_data):
                print("âœ… ç›´æ¥å­—ç¬¦ä¸²å…§å®¹")
                return api_data
                
        print("âŒ æœªèƒ½å¾APIå›æ‡‰ä¸­æå–å…§å®¹")
        return None
        
    def crawl_via_api(self, url, title=None):
        """é€šéAPIçˆ¬å–å…§å®¹"""
        print(f"ğŸŒ é€šéAPIçˆ¬å–å…§å®¹")
        print(f"ç¶²å€: {url}")
        print("=" * 50)
        
        # 1. åˆ†æURLçµæ§‹
        analysis = self.analyze_url_structure(url)
        
        if not analysis['book_id'] or not analysis['chapter_id']:
            print("âŒ ç„¡æ³•å¾URLä¸­æå–æ›¸ç±æˆ–ç« ç¯€ID")
            return False
            
        # 2. å˜—è©¦APIç«¯é»
        api_responses = self.try_api_endpoints(
            analysis['book_id'], 
            analysis['chapter_id']
        )
        
        if not api_responses:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„APIç«¯é»")
            return False
            
        # 3. å¾APIå›æ‡‰ä¸­æå–å…§å®¹
        content = None
        for response in api_responses:
            content = self.extract_content_from_api_response(response['data'])
            if content:
                print(f"âœ… å¾ {response['url']} ç²å–åˆ°å…§å®¹")
                break
                
        if not content:
            print("âŒ ç„¡æ³•å¾APIå›æ‡‰ä¸­æå–å…§å®¹")
            # å„²å­˜APIå›æ‡‰ä»¥ä¾›èª¿è©¦
            with open("api_debug.json", "w", encoding="utf-8") as f:
                json.dump(api_responses, f, ensure_ascii=False, indent=2)
            print("ğŸ’¾ APIå›æ‡‰å·²å„²å­˜ç‚º api_debug.json")
            return False
            
        # 4. æ¸…ç†å’Œå„²å­˜å…§å®¹
        # ä½¿ç”¨çˆ¶é¡çš„clean_textæ–¹æ³•
        import re
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        # ç§»é™¤HTMLæ¨™ç±¤æ®˜ç•™
        content = re.sub(r'<[^>]+>', '', content)
        # æ•´ç†æ®µè½
        content = re.sub(r'\n\s*\n', '\n\n', content)
        cleaned_content = content.strip()
        
        if not title:
            title = f"{analysis['book_id']}_{analysis['chapter_id']}_APIçˆ¬å–"
            
        filename = f"{title}.txt"
        self.save_text(cleaned_content, filename, "../docs/source_texts")
        
        print(f"âœ… APIçˆ¬å–æˆåŠŸ: {title}")
        print(f"å…§å®¹é•·åº¦: {len(cleaned_content)} å­—ç¬¦")
        
        return True

# ä½¿ç”¨ç¤ºä¾‹
def test_api_crawler():
    """æ¸¬è©¦APIçˆ¬èŸ²"""
    crawler = APICrawler()
    
    url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1?page_from=home_page&version=19"
    
    success = crawler.crawl_via_api(url, "æŠ±æœ´å­_å…§ç¯‡_APIçˆ¬å–")
    
    if success:
        print("\nğŸ‰ APIçˆ¬å–å®Œæˆï¼")
    else:
        print("\nâŒ APIçˆ¬å–å¤±æ•—")
        print("ğŸ’¡ æç¤ºï¼šæª¢æŸ¥ api_debug.json å¯ä»¥çœ‹åˆ°APIå›æ‡‰å…§å®¹")

if __name__ == "__main__":
    test_api_crawler()