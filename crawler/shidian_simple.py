#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå…¸å¤ç±ç¶²ç°¡åŒ–çˆ¬èŸ²

å°ˆé–€é‡å°æ‚¨çš„éœ€æ±‚è¨­è¨ˆçš„ç°¡å–®å¯¦ç”¨ç‰ˆæœ¬
"""

import requests

def safe_print(*args, **kwargs):
    """å®‰å…¨çš„æ‰“å°å‡½æ•¸ï¼Œè‡ªå‹•è™•ç†å°å…¥å•é¡Œ"""
    try:
        from core.unicode_handler import safe_print as _safe_print
        _safe_print(*args, **kwargs)
    except ImportError:
        try:
            import sys
            from pathlib import Path
            sys.path.append(str(Path(__file__).parent.parent))
            from core.unicode_handler import safe_print as _safe_print
            _safe_print(*args, **kwargs)
        except ImportError:
            print(*args, **kwargs)
    except Exception:
        print(*args, **kwargs)

import re
import json
from pathlib import Path
from core.unicode_handler import safe_print

class ShidianSimple:
    """åå…¸å¤ç±ç¶²ç°¡åŒ–çˆ¬èŸ²"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def extract_ids_from_url(self, url):
        """å¾URLæå–æ›¸ç±å’Œç« ç¯€ID"""
        book_match = re.search(r'/book/([^/]+)', url)
        chapter_match = re.search(r'/chapter/([^/?]+)', url)
        
        if book_match and chapter_match:
            return book_match.group(1), chapter_match.group(1)
        return None, None
        
    def try_api_endpoints(self, book_id, chapter_id):
        """å˜—è©¦ä¸åŒçš„APIç«¯é»"""
        base_url = "https://www.shidianguji.com"
        
        # å¯èƒ½çš„APIç«¯é»
        endpoints = [
            f"/api/book/{book_id}/chapter/{chapter_id}",
            f"/api/chapter/{chapter_id}",
            f"/api/content/{book_id}/{chapter_id}",
        ]
        
        results = []
        
        for endpoint in endpoints:
            try:
                url = base_url + endpoint
                safe_print(f"å˜—è©¦: {url}")
                
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    content = response.text
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰ç”¨çš„ä¸­æ–‡å…§å®¹
                    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
                    
                    if chinese_chars > 100:  # è‡³å°‘100å€‹ä¸­æ–‡å­—ç¬¦
                        safe_print(f"âœ… æˆåŠŸ: {url} (ä¸­æ–‡å­—ç¬¦: {chinese_chars})")
                        results.append({
                            'url': url,
                            'content': content,
                            'chinese_count': chinese_chars
                        })
                    else:
                        safe_print(f"âš ï¸  å›æ‡‰å¤ªçŸ­: {url}")
                else:
                    safe_print(f"âŒ å¤±æ•—: {url} (ç‹€æ…‹ç¢¼: {response.status_code})")
                    
            except Exception as e:
                safe_print(f"âŒ éŒ¯èª¤: {url} - {e}")
                
        return results
        
    def extract_text_content(self, content):
        """å¾å›æ‡‰ä¸­æå–æ–‡æœ¬å…§å®¹"""
        
        # æ–¹æ³•1: å°‹æ‰¾JSONä¸­çš„æ–‡æœ¬
        try:
            # å˜—è©¦è§£æç‚ºJSON
            data = json.loads(content)
            return self.extract_from_json(data)
        except:
            pass
            
        # æ–¹æ³•2: å¾HTML/JavaScriptä¸­æå–
        return self.extract_from_html_js(content)
        
    def extract_from_json(self, data):
        """å¾JSONæ•¸æ“šä¸­æå–æ–‡æœ¬"""
        texts = []
        
        def search_json(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key in ['content', 'text', 'body'] and isinstance(value, str):
                        if len(value) > 20 and re.search(r'[\u4e00-\u9fff]', value):
                            texts.append(value)
                    elif isinstance(value, (dict, list)):
                        search_json(value)
            elif isinstance(obj, list):
                for item in obj:
                    search_json(item)
                    
        search_json(data)
        return '\n\n'.join(texts) if texts else None
        
    def extract_from_html_js(self, content):
        """å¾HTML/JavaScriptä¸­æå–æ–‡æœ¬"""
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')
            
            # å°‹æ‰¾ä¸»è¦å…§å®¹å€åŸŸ
            main_content = soup.find('main', class_='read-layout-main')
            if main_content:
                article = main_content.find('article', class_='chapter-reader')
                if article:
                    # æå–æ‰€æœ‰æ®µè½å’Œæ¨™é¡Œ
                    texts = []
                    for element in article.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                        text = element.get_text().strip()
                        if text and len(text) > 5:  # éæ¿¾å¤ªçŸ­çš„æ–‡æœ¬
                            texts.append(text)
                    
                    if texts:
                        return '\n\n'.join(texts)
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šçµæ§‹ï¼Œå˜—è©¦æå–æ‰€æœ‰æœ‰æ„ç¾©çš„æ®µè½
            paragraphs = soup.find_all('p')
            meaningful_texts = []
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 20 and re.search(r'[\u4e00-\u9fff]', text):
                    meaningful_texts.append(text)
                    
            if meaningful_texts:
                return '\n\n'.join(meaningful_texts)
                
        except ImportError:
            safe_print("âš ï¸  éœ€è¦å®‰è£ beautifulsoup4: pip install beautifulsoup4")
        except Exception as e:
            safe_print(f"âš ï¸  HTMLè§£æéŒ¯èª¤: {e}")
        
        # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ­£è¦è¡¨é”å¼
        patterns = [
            r'"content":\s*"([^"]+)"',
            r'"text":\s*"([^"]+)"',
            r'content:\s*"([^"]+)"',
            r'text:\s*"([^"]+)"',
        ]
        
        all_matches = []
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                # è§£ç¢¼Unicodeè½‰ç¾©
                try:
                    decoded = match.encode().decode('unicode_escape')
                    if len(decoded) > 20 and re.search(r'[\u4e00-\u9fff]', decoded):
                        all_matches.append(decoded)
                except:
                    if len(match) > 20 and re.search(r'[\u4e00-\u9fff]', match):
                        all_matches.append(match)
                        
        return '\n\n'.join(all_matches) if all_matches else None
        
    def crawl(self, url, output_filename=None):
        """çˆ¬å–æŒ‡å®šURLçš„å…§å®¹"""
        safe_print(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å–: {url}")
        safe_print("=" * 50)
        
        # æå–ID
        book_id, chapter_id = self.extract_ids_from_url(url)
        if not book_id or not chapter_id:
            safe_print("âŒ ç„¡æ³•å¾URLæå–ID")
            return False
            
        safe_print(f"æ›¸ç±ID: {book_id}")
        safe_print(f"ç« ç¯€ID: {chapter_id}")
        
        # å˜—è©¦APIç«¯é»
        results = self.try_api_endpoints(book_id, chapter_id)
        
        if not results:
            safe_print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„APIç«¯é»")
            return False
            
        # é¸æ“‡æœ€ä½³çµæœï¼ˆä¸­æ–‡å­—ç¬¦æœ€å¤šçš„ï¼‰
        best_result = max(results, key=lambda x: x['chinese_count'])
        safe_print(f"\nğŸ“– ä½¿ç”¨æœ€ä½³çµæœ: {best_result['url']}")
        
        # æå–æ–‡æœ¬å…§å®¹
        text_content = self.extract_text_content(best_result['content'])
        
        if not text_content:
            safe_print("âŒ ç„¡æ³•æå–æ–‡æœ¬å…§å®¹")
            # å„²å­˜åŸå§‹å…§å®¹ä»¥ä¾›èª¿è©¦
            debug_file = f"debug_{book_id}_{chapter_id}.txt"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(best_result['content'])
            safe_print(f"ğŸ’¾ åŸå§‹å…§å®¹å·²å„²å­˜ç‚º: {debug_file}")
            return False
            
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self.clean_text(text_content)
        
        # å„²å­˜çµæœ
        if not output_filename:
            output_filename = f"{book_id}_{chapter_id}_çˆ¬å–çµæœ.txt"
            
        output_path = Path("../docs/source_texts") / output_filename
        output_path.parent.mkdir(exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(cleaned_text)
            
        safe_print(f"âœ… çˆ¬å–æˆåŠŸ!")
        safe_print(f"æ–‡ä»¶: {output_path}")
        safe_print(f"å…§å®¹é•·åº¦: {len(cleaned_text)} å­—ç¬¦")
        
        return True
        
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤HTMLæ¨™ç±¤
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤JavaScriptä»£ç¢¼ç‰‡æ®µ
        text = re.sub(r'function\s*\([^)]*\)\s*\{[^}]*\}', '', text)
        text = re.sub(r'var\s+\w+\s*=\s*[^;]+;', '', text)
        # æ•´ç†æ®µè½
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()

def main():
    """ä¸»å‡½æ•¸"""
    crawler = ShidianSimple()
    
    # æ‚¨æä¾›çš„URL
    url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1?page_from=home_page&version=19"
    
    success = crawler.crawl(url, "æŠ±æœ´å­_ç¬¬ä¸€ç« _ç°¡åŒ–ç‰ˆ.txt")
    
    if success:
        safe_print("\nğŸ‰ çˆ¬å–å®Œæˆ!")
        safe_print("ğŸ’¡ æç¤º: å¦‚æœå…§å®¹ä¸å®Œæ•´ï¼Œå¯ä»¥æª¢æŸ¥debugæ–‡ä»¶")
    else:
        safe_print("\nâŒ çˆ¬å–å¤±æ•—")
        safe_print("ğŸ’¡ æç¤º: æª¢æŸ¥debugæ–‡ä»¶äº†è§£è©³ç´°æƒ…æ³")

if __name__ == "__main__":
    main()
