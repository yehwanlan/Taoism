# çˆ¬èŸ²æ¨¡çµ„å¿«é€Ÿåƒè€ƒ

## ğŸš€ ä¸€åˆ†é˜ä¸Šæ‰‹

```python
from crawler import crawl_book

book = crawl_book('DZ1422')
```

## ğŸ“š å°å…¥æ–¹å¼

```python
# æ–¹å¼1: æ¨™æº–å°å…¥ï¼ˆæ¨è–¦ï¼‰
from crawler import ShidianCrawler
crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1422')

# æ–¹å¼2: ä¾¿æ·å‡½æ•¸
from crawler import crawl_book
book = crawl_book('DZ1422')

# æ–¹å¼3: æ‰¹é‡çˆ¬å–
from crawler import batch_crawl
results = batch_crawl(['DZ1422', 'DZ1439'])
```

## ğŸ¯ å¸¸ç”¨æ“ä½œ

### çˆ¬å–å–®æœ¬æ›¸ç±

```python
from crawler import ShidianCrawler

crawler = ShidianCrawler(delay=2)
book = crawler.crawl_book('DZ1422')
```

### ä¸ç”Ÿæˆç¿»è­¯æ¨¡æ¿

```python
book = crawler.crawl_book('DZ1422', generate_templates=False)
```

### æ‰¹é‡çˆ¬å–

```python
crawler = ShidianCrawler(delay=3)
results = crawler.batch_crawl(['DZ1422', 'DZ1439'])
```

### è‡ªè¨‚è¼¸å‡ºç›®éŒ„

```python
crawler.save_to_text_files(book, 'my_output/texts')
crawler.generate_translation_templates(book, 'my_output/translations')
```

## ğŸ“Š è¼¸å‡ºçµæ§‹

```
docs/
â”œâ”€â”€ source_texts/æ›¸å/      # åŸæ–‡
â””â”€â”€ translations/æ›¸å/      # ç¿»è­¯æ¨¡æ¿

data/
â””â”€â”€ crawled/               # JSONè³‡æ–™
```

## ğŸ”§ åƒæ•¸è¨­å®š

```python
# å»¶é²æ™‚é–“
ShidianCrawler(delay=2)     # é è¨­
ShidianCrawler(delay=3)     # æ‰¹é‡çˆ¬å–å»ºè­°

# ç”Ÿæˆç¿»è­¯æ¨¡æ¿
crawl_book('DZ1422', generate_templates=True)   # é è¨­
crawl_book('DZ1422', generate_templates=False)  # ä¸ç”Ÿæˆ
```

## ğŸ“ API é€ŸæŸ¥

### ShidianCrawler é¡åˆ¥

```python
crawler = ShidianCrawler(delay=2)
book = crawler.get_book_info(book_id)
content = crawler.get_chapter_content(url, name)
book = crawler.crawl_all_chapters(book_info)
book = crawler.crawl_book(book_id, generate_templates=True)
crawler.save_to_json(book, output_dir)
crawler.save_to_text_files(book, output_dir)
crawler.generate_translation_templates(book, output_dir)
results = crawler.batch_crawl(book_ids, output_dir)
crawler.print_statistics(book)
```

### ä¾¿æ·å‡½æ•¸

```python
book = crawl_book(book_id, delay=2, generate_templates=True)
results = batch_crawl(book_ids, delay=3, output_dir='data/crawled')
```

## ğŸ¯ æ›¸ç±ç·¨è™Ÿ

å¾ URL ä¸­æ‰¾ï¼š
```
https://www.shidianguji.com/book/DZ1422
                                  ^^^^^^
```

å¸¸è¦‹ç·¨è™Ÿï¼š
- `DZ1422` - æ•ä¸­ç»
- `DZ1439` - æ´ç„çµå®ç‰äº¬å±±æ­¥è™šç»

## â“ å¸¸è¦‹å•é¡Œ

**Q: çˆ¬å–å¤±æ•—ï¼Ÿ**  
A: æª¢æŸ¥ç¶²è·¯ã€æ›¸ç±ç·¨è™Ÿã€æŸ¥çœ‹æ—¥èªŒ `data/logs/shidian_crawler.log`

**Q: ç¿»è­¯æ¨¡æ¿æ²’ç”Ÿæˆï¼Ÿ**  
A: ç¢ºèª `generate_templates=True`

**Q: å¦‚ä½•æ‰¹é‡çˆ¬å–ï¼Ÿ**  
A: ä½¿ç”¨ `batch_crawl(['DZ1422', 'DZ1439'])`

## ğŸ“š å®Œæ•´æ–‡æª”

- [README.md](README.md) - æ¨¡çµ„ç¸½è¦½
- [å¿«é€Ÿé–‹å§‹.md](å¿«é€Ÿé–‹å§‹.md) - è©³ç´°æ•™å­¸
- [README_æ›´æ–°èªªæ˜.md](README_æ›´æ–°èªªæ˜.md) - APIæ–‡æª”

## ğŸ‰ ç¯„ä¾‹

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1422')

if book:
    crawler.print_statistics(book)
    print(f"âœ“ å®Œæˆ: {book['title']}")
```

---

**å¿«é€Ÿåƒè€ƒç‰ˆæœ¬**: v2.0  
**æ›´æ–°æ™‚é–“**: 2025-10-20
