#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çˆ¬èŸ² - é‡å°åå…¸å¤ç±ç¶²çš„å„ªåŒ–ç‰ˆæœ¬

å­¸ç¿’é‡é»ï¼š
1. å¦‚ä½•åˆ†æç¶²é çš„å¯¦éš›HTMLçµæ§‹
2. å¦‚ä½•æ‰¾åˆ°éš±è—åœ¨è¤‡é›œçµæ§‹ä¸­çš„å…§å®¹
3. å¦‚ä½•è™•ç†ç‰¹æ®Šçš„ç¶²ç«™æ¶æ§‹
4. å¦‚ä½•æå–ç´”æ–‡æœ¬å…§å®¹
"""

import re
import json
from base_crawler import BaseCrawler

class SmartCrawler(BaseCrawler):
    """æ™ºèƒ½çˆ¬èŸ²"""
    
    def __init__(self):
        super().__init__(delay_range=(2, 4))
        
    def extract_raw_html(self, url):
        """æå–åŸå§‹HTMLä¸¦åˆ†æ"""
        print(f"ğŸ” æå–åŸå§‹HTML: {url}")
        
        response = self.make_request(url)
        if not response:
            return None
            
        # å„²å­˜åŸå§‹HTMLä»¥ä¾›åˆ†æ
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(response.text)
        print("ğŸ’¾ åŸå§‹HTMLå·²å„²å­˜ç‚º debug_page.html")
        
        return response.text
        
    def find_content_in_html(self, html_content):
        """åœ¨HTMLä¸­å°‹æ‰¾å¯¦éš›å…§å®¹"""
        print("ğŸ” åœ¨HTMLä¸­æœå°‹å…§å®¹...")
        
        soup = self.parse_html(html_content)
        
        # æ–¹æ³•1: å°‹æ‰¾æ‰€æœ‰æ–‡æœ¬ç¯€é»
        all_text = soup.get_text()
        print(f"ç¸½æ–‡æœ¬é•·åº¦: {len(all_text)} å­—ç¬¦")
        
        # æ–¹æ³•2: å°‹æ‰¾åŒ…å«ä¸­æ–‡çš„é•·æ®µè½
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        paragraphs = []
        
        for element in soup.find_all(['p', 'div', 'span', 'td']):
            text = element.get_text().strip()
            if len(text) > 30 and chinese_pattern.search(text):
                paragraphs.append({
                    'text': text,
                    'length': len(text),
                    'tag': element.name,
                    'class': element.get('class', []),
                    'id': element.get('id', '')
                })
        
        # æŒ‰é•·åº¦æ’åº
        paragraphs.sort(key=lambda x: x['length'], reverse=True)
        
        print(f"æ‰¾åˆ° {len(paragraphs)} å€‹ä¸­æ–‡æ®µè½")
        
        # é¡¯ç¤ºæœ€é•·çš„å¹¾å€‹æ®µè½
        for i, para in enumerate(paragraphs[:5]):
            print(f"æ®µè½ {i+1}: {para['length']} å­—ç¬¦")
            print(f"  æ¨™ç±¤: {para['tag']}")
            if para['class']:
                print(f"  é¡åˆ¥: {' '.join(para['class'])}")
            if para['id']:
                print(f"  ID: {para['id']}")
            print(f"  é è¦½: {para['text'][:100]}...")
            print()
            
        return paragraphs
        
    def extract_book_content(self, paragraphs):
        """å¾æ®µè½ä¸­æå–æ›¸ç±å…§å®¹"""
        print("ğŸ“– æå–æ›¸ç±å…§å®¹...")
        
        # éæ¿¾æ‰å°èˆªã€ç‰ˆæ¬Šç­‰ç„¡é—œå…§å®¹
        filter_keywords = [
            'ç‰ˆæ¬Š', 'å°èˆª', 'èœå–®', 'ç™»éŒ„', 'è¨»å†Š', 'æœç´¢', 
            'é¦–é ', 'é—œæ–¼', 'è¯ç¹«', 'å‹æƒ…éˆæ¥', 'copyright',
            'ç½‘ç«™', 'å¹³å°', 'æœåŠ¡', 'ç”¨æˆ·', 'éšç§', 'æ¡æ¬¾'
        ]
        
        content_paragraphs = []
        for para in paragraphs:
            text = para['text']
            # æª¢æŸ¥æ˜¯å¦åŒ…å«éæ¿¾é—œéµå­—
            if not any(keyword in text for keyword in filter_keywords):
                # æª¢æŸ¥æ˜¯å¦æ˜¯å¯¦éš›çš„æ›¸ç±å…§å®¹ï¼ˆåŒ…å«å¤æ–‡ç‰¹å¾µï¼‰
                if self.is_classical_text(text):
                    content_paragraphs.append(para)
                    
        print(f"éæ¿¾å¾Œå‰©é¤˜ {len(content_paragraphs)} å€‹å…§å®¹æ®µè½")
        
        if content_paragraphs:
            # åˆä½µå…§å®¹
            final_content = '\n\n'.join([para['text'] for para in content_paragraphs])
            return self.clean_text(final_content)
        
        return None
        
    def is_classical_text(self, text):
        """åˆ¤æ–·æ˜¯å¦ç‚ºå¤å…¸æ–‡æœ¬"""
        # å¤æ–‡ç‰¹å¾µæª¢æŸ¥
        classical_indicators = [
            'ä¹‹', 'è€…', 'ä¹Ÿ', 'çŸ£', 'ç„‰', 'ä¹', 'å“‰', 'è€³',
            'æ›°', 'äº‘', 'è¬‚', 'æ•…', 'æ˜¯ä»¥', 'ç„¶å‰‡', 'å¤«',
            'è“‹', 'ä¸”', 'è‹¥', 'å‰‡', 'è€Œ', 'ä»¥', 'æ–¼'
        ]
        
        # è¨ˆç®—å¤æ–‡æŒ‡æ¨™å‡ºç¾æ¬¡æ•¸
        indicator_count = sum(1 for indicator in classical_indicators if indicator in text)
        
        # å¦‚æœå¤æ–‡æŒ‡æ¨™è¶³å¤ å¤šï¼Œä¸”æ–‡æœ¬é•·åº¦åˆç†ï¼Œå‰‡èªç‚ºæ˜¯å¤å…¸æ–‡æœ¬
        return indicator_count >= 3 and len(text) > 50
        
    def crawl_shidian_smart(self, url, title=None):
        """æ™ºèƒ½çˆ¬å–åå…¸å¤ç±"""
        print(f"ğŸ§  æ™ºèƒ½çˆ¬å–é–‹å§‹")
        print(f"ç¶²å€: {url}")
        print("=" * 50)
        
        # 1. æå–åŸå§‹HTML
        html_content = self.extract_raw_html(url)
        if not html_content:
            return False
            
        # 2. åˆ†æHTMLçµæ§‹
        paragraphs = self.find_content_in_html(html_content)
        if not paragraphs:
            return False
            
        # 3. æå–æ›¸ç±å…§å®¹
        content = self.extract_book_content(paragraphs)
        if not content:
            print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„æ›¸ç±å…§å®¹")
            return False
            
        # 4. ç¢ºå®šæ¨™é¡Œ
        if not title:
            soup = self.parse_html(html_content)
            page_title = soup.find('title')
            if page_title:
                title = page_title.get_text().split('-')[0].strip()
            else:
                title = "åå…¸å¤ç±_æ™ºèƒ½çˆ¬å–"
                
        # 5. å„²å­˜å…§å®¹
        filename = f"{title}.txt"
        self.save_text(content, filename, "../docs/source_texts")
        
        print(f"âœ… æ™ºèƒ½çˆ¬å–æˆåŠŸ: {title}")
        print(f"å…§å®¹é•·åº¦: {len(content)} å­—ç¬¦")
        print(f"å·²å„²å­˜ç‚º: {filename}")
        
        return True

# ä½¿ç”¨ç¤ºä¾‹
def test_smart_crawler():
    """æ¸¬è©¦æ™ºèƒ½çˆ¬èŸ²"""
    crawler = SmartCrawler()
    
    url = "https://www.shidianguji.com/book/DZ0095/chapter/DZ0095_1?page_from=bookshelf&version=2"
    
    success = crawler.crawl_shidian_smart(url, "DZ0095_æ™ºèƒ½çˆ¬å–")
    
    if success:
        print("\nğŸ‰ æ™ºèƒ½çˆ¬å–å®Œæˆï¼")
        print("ğŸ’¡ æç¤ºï¼šæª¢æŸ¥ debug_page.html å¯ä»¥çœ‹åˆ°å®Œæ•´çš„ç¶²é çµæ§‹")
    else:
        print("\nâŒ æ™ºèƒ½çˆ¬å–å¤±æ•—")

if __name__ == "__main__":
    test_smart_crawler()