#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ±æœ´å­å°ˆç”¨çˆ¬èŸ²

å°ˆé–€ç”¨æ–¼çˆ¬å–æŠ±æœ´å­å„ç« ç¯€å…§å®¹
"""

import requests
import re
from pathlib import Path
from bs4 import BeautifulSoup

class BaopuziCrawler:
    """æŠ±æœ´å­å°ˆç”¨çˆ¬èŸ²"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.base_url = "https://www.shidianguji.com"
        
    def extract_ids_from_url(self, url):
        """å¾URLæå–æ›¸ç±å’Œç« ç¯€ID"""
        book_match = re.search(r'/book/([^/]+)', url)
        chapter_match = re.search(r'/chapter/([^/?]+)', url)
        
        if book_match and chapter_match:
            return book_match.group(1), chapter_match.group(1)
        return None, None
        
    def get_chapter_content(self, url):
        """ç²å–ç« ç¯€å…§å®¹"""
        book_id, chapter_id = self.extract_ids_from_url(url)
        if not book_id or not chapter_id:
            print("âŒ ç„¡æ³•å¾URLæå–ID")
            return None, None
            
        # ä½¿ç”¨APIç«¯é»
        api_url = f"{self.base_url}/api/book/{book_id}/chapter/{chapter_id}"
        
        try:
            response = self.session.get(api_url, timeout=10)
            if response.status_code == 200:
                return self.extract_text_from_html(response.text), chapter_id
        except Exception as e:
            print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")
            
        return None, None
        
    def extract_text_from_html(self, html_content):
        """å¾HTMLä¸­æå–æ–‡æœ¬å…§å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # å°‹æ‰¾ä¸»è¦å…§å®¹å€åŸŸ
            main_content = soup.find('main', class_='read-layout-main')
            if main_content:
                article = main_content.find('article', class_='chapter-reader')
                if article:
                    # æå–æ¨™é¡Œå’Œå…§å®¹
                    result = {
                        'title': '',
                        'content': []
                    }
                    
                    for element in article.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):
                        text = element.get_text().strip()
                        if text and len(text) > 3:
                            if element.name.startswith('h'):
                                result['title'] = text
                            else:
                                result['content'].append(text)
                    
                    return result
                    
        except Exception as e:
            print(f"âš ï¸  HTMLè§£æéŒ¯èª¤: {e}")
            
        return None
        
    def save_chapter(self, content_data, chapter_id, chapter_number=None):
        """å„²å­˜ç« ç¯€å…§å®¹"""
        if not content_data:
            return False
            
        # å»ºç«‹æª”æ¡ˆåç¨±
        if chapter_number:
            filename = f"{chapter_number:02d}_{content_data['title']}.txt"
        else:
            filename = f"{content_data['title']}.txt"
            
        # æ¸…ç†æª”æ¡ˆåç¨±ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        # å»ºç«‹å®Œæ•´å…§å®¹
        full_content = f"# {content_data['title']}\n\n"
        full_content += '\n\n'.join(content_data['content'])
        
        # å„²å­˜åˆ°æŠ±æœ´å­è³‡æ–™å¤¾
        output_path = Path("../docs/source_texts/æŠ±æœ´å­/åŸæ–‡") / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
            
        print(f"âœ… å·²å„²å­˜: {filename}")
        print(f"   æ¨™é¡Œ: {content_data['title']}")
        print(f"   å…§å®¹é•·åº¦: {len(full_content)} å­—ç¬¦")
        
        return True
        
    def crawl_chapter(self, url, chapter_number=None):
        """çˆ¬å–å–®ä¸€ç« ç¯€"""
        print(f"ğŸ•·ï¸ çˆ¬å–ç« ç¯€: {url}")
        
        content_data, chapter_id = self.get_chapter_content(url)
        
        if content_data:
            return self.save_chapter(content_data, chapter_id, chapter_number)
        else:
            print("âŒ ç„¡æ³•ç²å–ç« ç¯€å…§å®¹")
            return False

def main():
    """ä¸»å‡½æ•¸"""
    crawler = BaopuziCrawler()
    
    # æŠ±æœ´å­ç« ç¯€åˆ—è¡¨
    chapters = [
        {
            'number': 2,
            'title': 'æŠ±æœ´å­åº',
            'url': 'https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwyyqgje_2?page_from=home_page&version=19'
        },
        # å¯ä»¥æ·»åŠ æ›´å¤šç« ç¯€...
    ]
    
    print("ğŸ—ï¸ é–‹å§‹å»ºç«‹æŠ±æœ´å­å°ˆæ¡ˆ")
    print("=" * 50)
    
    success_count = 0
    for chapter in chapters:
        print(f"\nğŸ“– è™•ç†ç¬¬ {chapter['number']} ç« : {chapter['title']}")
        
        if crawler.crawl_chapter(chapter['url'], chapter['number']):
            success_count += 1
        else:
            print(f"âŒ ç¬¬ {chapter['number']} ç« çˆ¬å–å¤±æ•—")
            
    print(f"\nğŸ‰ å®Œæˆï¼æˆåŠŸçˆ¬å– {success_count}/{len(chapters)} ç« ")

if __name__ == "__main__":
    main()