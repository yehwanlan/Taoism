# 師典古籍爬蟲 - 快速開始

## 🚀 5 分鐘上手

### 1. 最簡單的方式

```bash
# 直接執行，爬取 DZ1439
python crawler/shidian_crawler.py
```

### 2. 爬取其他書籍

```python
from crawler.shidian_crawler import ShidianCrawler

# 建立爬蟲
crawler = ShidianCrawler()

# 爬取你想要的書籍（改這裡的編號）
book_info = crawler.crawl_book('DZ1234')  # 改成你要的書籍編號

# 自動保存
crawler.save_to_json(book_info)
crawler.save_to_text_files(book_info)

# 查看統計
crawler.print_statistics(book_info)
```

### 3. 批量爬取多本書

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()

# 一次爬取多本（改這裡的編號列表）
book_ids = ['DZ1439', 'DZ1234', 'DZ1437']
results = crawler.batch_crawl(book_ids)

print(f"成功爬取 {len(results)} 本書籍")
```

## 📁 輸出位置

### 文字檔案
```
docs/source_texts/
└── 書名/
    ├── 00_書籍資訊.txt
    ├── 01_章節1.txt
    ├── 02_章節2.txt
    └── ...
```

### JSON 檔案
```
data/crawled/
└── DZ1439_書名.json
```

### 日誌檔案
```
data/logs/
└── shidian_crawler.log
```

## 🎯 常見使用場景

### 場景1: 我只想爬一本書

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1439')
crawler.save_to_json(book)
crawler.save_to_text_files(book)
```

### 場景2: 我想爬很多書

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler(delay=3)  # 延遲 3 秒更安全

book_ids = [
    'DZ1439',
    'DZ1234', 
    'DZ1437',
    # ... 添加更多
]

results = crawler.batch_crawl(book_ids)
```

### 場景3: 我只想要 JSON，不要文字檔案

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1439')
crawler.save_to_json(book)  # 只保存 JSON
```

### 場景4: 我想自訂輸出位置

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1439')

# 自訂輸出位置
crawler.save_to_json(book, 'my_output/json')
crawler.save_to_text_files(book, 'my_output/texts')
```

## ⚙️ 參數調整

### 調整延遲時間

```python
# 預設 2 秒
crawler = ShidianCrawler(delay=2)

# 更保守（推薦）
crawler = ShidianCrawler(delay=3)

# 更快速（不建議）
crawler = ShidianCrawler(delay=1)
```

## 📊 查看結果

### 方法1: 列印統計

```python
crawler.print_statistics(book_info)
```

輸出：
```
============================================================
爬取統計
============================================================
書名: 洞玄灵宝玉京山步虚经
作者: 佚名
朝代: 东晋
總章節數: 7
成功爬取: 7 章
失敗: 0 章
總字數: 3,502 字
平均每章: 500 字
============================================================
```

### 方法2: 查看 JSON

```python
import json

with open('data/crawled/DZ1439_洞玄灵宝玉京山步虚经.json', 'r', encoding='utf-8') as f:
    book = json.load(f)
    print(f"書名: {book['title']}")
    print(f"章節數: {len(book['chapters'])}")
```

### 方法3: 查看文字檔案

直接打開 `docs/source_texts/書名/` 目錄中的 txt 檔案

## ❓ 常見問題

### Q1: 如何找到書籍編號？

A: 從師典古籍網站的 URL 中找，例如：
- `https://www.shidianguji.com/book/DZ1439` → 編號是 `DZ1439`

### Q2: 爬取失敗怎麼辦？

A: 檢查：
1. 網路連線是否正常
2. 書籍編號是否正確
3. 查看日誌檔案 `data/logs/shidian_crawler.log`

### Q3: 可以同時爬取多本書嗎？

A: 可以！使用 `batch_crawl()` 方法：
```python
results = crawler.batch_crawl(['DZ1439', 'DZ1234'])
```

### Q4: 爬取速度太慢？

A: 可以調整延遲時間，但不建議低於 1 秒：
```python
crawler = ShidianCrawler(delay=1)  # 最低建議值
```

### Q5: 如何只爬取章節列表，不爬內容？

A: 使用 `get_book_info()` 方法：
```python
book_info = crawler.get_book_info('DZ1439')
# 這只會獲取書籍資訊和章節列表，不會爬取內容
```

## 🎓 進階使用

### 只爬取特定章節

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()

# 獲取書籍資訊
book_info = crawler.get_book_info('DZ1439')

# 只爬取第 1、3、5 章
for i in [0, 2, 4]:  # 索引從 0 開始
    chapter = book_info['chapters'][i]
    content = crawler.get_chapter_content(chapter['url'], chapter['name'])
    if content:
        chapter['content'] = content['content']
```

### 自訂錯誤處理

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()

try:
    book = crawler.crawl_book('DZ1439')
    if book:
        crawler.save_to_json(book)
        print("成功！")
    else:
        print("爬取失敗")
except Exception as e:
    print(f"發生錯誤: {e}")
```

## 📝 完整範例

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
完整的爬蟲使用範例
"""

from crawler.shidian_crawler import ShidianCrawler

def main():
    # 1. 建立爬蟲實例
    crawler = ShidianCrawler(delay=2)
    
    # 2. 爬取書籍
    print("開始爬取...")
    book_info = crawler.crawl_book('DZ1439')
    
    # 3. 檢查結果
    if not book_info:
        print("爬取失敗！")
        return
    
    # 4. 保存結果
    print("\n保存結果...")
    crawler.save_to_json(book_info)
    crawler.save_to_text_files(book_info)
    
    # 5. 顯示統計
    print("\n統計資訊:")
    crawler.print_statistics(book_info)
    
    print("\n✓ 完成！")

if __name__ == "__main__":
    main()
```

## ✅ 檢查清單

使用前確認：
- [ ] 已安裝 Python 3.7+
- [ ] 已安裝依賴套件（requests, beautifulsoup4）
- [ ] 網路連線正常
- [ ] 有足夠的磁碟空間

使用後檢查：
- [ ] 檢查 `docs/source_texts/` 是否有文字檔案
- [ ] 檢查 `data/crawled/` 是否有 JSON 檔案
- [ ] 查看日誌檔案確認沒有錯誤
- [ ] 驗證內容是否完整

## 🎉 開始使用！

現在你已經準備好了，開始爬取你的第一本道教經典吧！

```bash
python crawler/shidian_crawler.py
```
