# å¸«å…¸å¤ç±çˆ¬èŸ² - å¿«é€Ÿé–‹å§‹

## ğŸš€ 5 åˆ†é˜ä¸Šæ‰‹

### 1. æœ€ç°¡å–®çš„æ–¹å¼

```bash
# ç›´æ¥åŸ·è¡Œï¼Œçˆ¬å– DZ1439
python crawler/shidian_crawler.py
```

### 2. çˆ¬å–å…¶ä»–æ›¸ç±

```python
from crawler.shidian_crawler import ShidianCrawler

# å»ºç«‹çˆ¬èŸ²
crawler = ShidianCrawler()

# çˆ¬å–ä½ æƒ³è¦çš„æ›¸ç±ï¼ˆæ”¹é€™è£¡çš„ç·¨è™Ÿï¼‰
book_info = crawler.crawl_book('DZ1234')  # æ”¹æˆä½ è¦çš„æ›¸ç±ç·¨è™Ÿ

# è‡ªå‹•ä¿å­˜
crawler.save_to_json(book_info)
crawler.save_to_text_files(book_info)

# æŸ¥çœ‹çµ±è¨ˆ
crawler.print_statistics(book_info)
```

### 3. æ‰¹é‡çˆ¬å–å¤šæœ¬æ›¸

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()

# ä¸€æ¬¡çˆ¬å–å¤šæœ¬ï¼ˆæ”¹é€™è£¡çš„ç·¨è™Ÿåˆ—è¡¨ï¼‰
book_ids = ['DZ1439', 'DZ1234', 'DZ1437']
results = crawler.batch_crawl(book_ids)

print(f"æˆåŠŸçˆ¬å– {len(results)} æœ¬æ›¸ç±")
```

## ğŸ“ è¼¸å‡ºä½ç½®

### æ–‡å­—æª”æ¡ˆ
```
docs/source_texts/
â””â”€â”€ æ›¸å/
    â”œâ”€â”€ 00_æ›¸ç±è³‡è¨Š.txt
    â”œâ”€â”€ 01_ç« ç¯€1.txt
    â”œâ”€â”€ 02_ç« ç¯€2.txt
    â””â”€â”€ ...
```

### JSON æª”æ¡ˆ
```
data/crawled/
â””â”€â”€ DZ1439_æ›¸å.json
```

### æ—¥èªŒæª”æ¡ˆ
```
data/logs/
â””â”€â”€ shidian_crawler.log
```

## ğŸ¯ å¸¸è¦‹ä½¿ç”¨å ´æ™¯

### å ´æ™¯1: æˆ‘åªæƒ³çˆ¬ä¸€æœ¬æ›¸

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1439')
crawler.save_to_json(book)
crawler.save_to_text_files(book)
```

### å ´æ™¯2: æˆ‘æƒ³çˆ¬å¾ˆå¤šæ›¸

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler(delay=3)  # å»¶é² 3 ç§’æ›´å®‰å…¨

book_ids = [
    'DZ1439',
    'DZ1234', 
    'DZ1437',
    # ... æ·»åŠ æ›´å¤š
]

results = crawler.batch_crawl(book_ids)
```

### å ´æ™¯3: æˆ‘åªæƒ³è¦ JSONï¼Œä¸è¦æ–‡å­—æª”æ¡ˆ

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1439')
crawler.save_to_json(book)  # åªä¿å­˜ JSON
```

### å ´æ™¯4: æˆ‘æƒ³è‡ªè¨‚è¼¸å‡ºä½ç½®

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()
book = crawler.crawl_book('DZ1439')

# è‡ªè¨‚è¼¸å‡ºä½ç½®
crawler.save_to_json(book, 'my_output/json')
crawler.save_to_text_files(book, 'my_output/texts')
```

## âš™ï¸ åƒæ•¸èª¿æ•´

### èª¿æ•´å»¶é²æ™‚é–“

```python
# é è¨­ 2 ç§’
crawler = ShidianCrawler(delay=2)

# æ›´ä¿å®ˆï¼ˆæ¨è–¦ï¼‰
crawler = ShidianCrawler(delay=3)

# æ›´å¿«é€Ÿï¼ˆä¸å»ºè­°ï¼‰
crawler = ShidianCrawler(delay=1)
```

## ğŸ“Š æŸ¥çœ‹çµæœ

### æ–¹æ³•1: åˆ—å°çµ±è¨ˆ

```python
crawler.print_statistics(book_info)
```

è¼¸å‡ºï¼š
```
============================================================
çˆ¬å–çµ±è¨ˆ
============================================================
æ›¸å: æ´ç„çµå®ç‰äº¬å±±æ­¥è™šç»
ä½œè€…: ä½šå
æœä»£: ä¸œæ™‹
ç¸½ç« ç¯€æ•¸: 7
æˆåŠŸçˆ¬å–: 7 ç« 
å¤±æ•—: 0 ç« 
ç¸½å­—æ•¸: 3,502 å­—
å¹³å‡æ¯ç« : 500 å­—
============================================================
```

### æ–¹æ³•2: æŸ¥çœ‹ JSON

```python
import json

with open('data/crawled/DZ1439_æ´ç„çµå®ç‰äº¬å±±æ­¥è™šç».json', 'r', encoding='utf-8') as f:
    book = json.load(f)
    print(f"æ›¸å: {book['title']}")
    print(f"ç« ç¯€æ•¸: {len(book['chapters'])}")
```

### æ–¹æ³•3: æŸ¥çœ‹æ–‡å­—æª”æ¡ˆ

ç›´æ¥æ‰“é–‹ `docs/source_texts/æ›¸å/` ç›®éŒ„ä¸­çš„ txt æª”æ¡ˆ

## â“ å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•æ‰¾åˆ°æ›¸ç±ç·¨è™Ÿï¼Ÿ

A: å¾å¸«å…¸å¤ç±ç¶²ç«™çš„ URL ä¸­æ‰¾ï¼Œä¾‹å¦‚ï¼š
- `https://www.shidianguji.com/book/DZ1439` â†’ ç·¨è™Ÿæ˜¯ `DZ1439`

### Q2: çˆ¬å–å¤±æ•—æ€éº¼è¾¦ï¼Ÿ

A: æª¢æŸ¥ï¼š
1. ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸
2. æ›¸ç±ç·¨è™Ÿæ˜¯å¦æ­£ç¢º
3. æŸ¥çœ‹æ—¥èªŒæª”æ¡ˆ `data/logs/shidian_crawler.log`

### Q3: å¯ä»¥åŒæ™‚çˆ¬å–å¤šæœ¬æ›¸å—ï¼Ÿ

A: å¯ä»¥ï¼ä½¿ç”¨ `batch_crawl()` æ–¹æ³•ï¼š
```python
results = crawler.batch_crawl(['DZ1439', 'DZ1234'])
```

### Q4: çˆ¬å–é€Ÿåº¦å¤ªæ…¢ï¼Ÿ

A: å¯ä»¥èª¿æ•´å»¶é²æ™‚é–“ï¼Œä½†ä¸å»ºè­°ä½æ–¼ 1 ç§’ï¼š
```python
crawler = ShidianCrawler(delay=1)  # æœ€ä½å»ºè­°å€¼
```

### Q5: å¦‚ä½•åªçˆ¬å–ç« ç¯€åˆ—è¡¨ï¼Œä¸çˆ¬å…§å®¹ï¼Ÿ

A: ä½¿ç”¨ `get_book_info()` æ–¹æ³•ï¼š
```python
book_info = crawler.get_book_info('DZ1439')
# é€™åªæœƒç²å–æ›¸ç±è³‡è¨Šå’Œç« ç¯€åˆ—è¡¨ï¼Œä¸æœƒçˆ¬å–å…§å®¹
```

## ğŸ“ é€²éšä½¿ç”¨

### åªçˆ¬å–ç‰¹å®šç« ç¯€

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()

# ç²å–æ›¸ç±è³‡è¨Š
book_info = crawler.get_book_info('DZ1439')

# åªçˆ¬å–ç¬¬ 1ã€3ã€5 ç« 
for i in [0, 2, 4]:  # ç´¢å¼•å¾ 0 é–‹å§‹
    chapter = book_info['chapters'][i]
    content = crawler.get_chapter_content(chapter['url'], chapter['name'])
    if content:
        chapter['content'] = content['content']
```

### è‡ªè¨‚éŒ¯èª¤è™•ç†

```python
from crawler.shidian_crawler import ShidianCrawler

crawler = ShidianCrawler()

try:
    book = crawler.crawl_book('DZ1439')
    if book:
        crawler.save_to_json(book)
        print("æˆåŠŸï¼")
    else:
        print("çˆ¬å–å¤±æ•—")
except Exception as e:
    print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
```

## ğŸ“ å®Œæ•´ç¯„ä¾‹

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„çˆ¬èŸ²ä½¿ç”¨ç¯„ä¾‹
"""

from crawler.shidian_crawler import ShidianCrawler

def main():
    # 1. å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
    crawler = ShidianCrawler(delay=2)
    
    # 2. çˆ¬å–æ›¸ç±
    print("é–‹å§‹çˆ¬å–...")
    book_info = crawler.crawl_book('DZ1439')
    
    # 3. æª¢æŸ¥çµæœ
    if not book_info:
        print("çˆ¬å–å¤±æ•—ï¼")
        return
    
    # 4. ä¿å­˜çµæœ
    print("\nä¿å­˜çµæœ...")
    crawler.save_to_json(book_info)
    crawler.save_to_text_files(book_info)
    
    # 5. é¡¯ç¤ºçµ±è¨ˆ
    print("\nçµ±è¨ˆè³‡è¨Š:")
    crawler.print_statistics(book_info)
    
    print("\nâœ“ å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

## âœ… æª¢æŸ¥æ¸…å–®

ä½¿ç”¨å‰ç¢ºèªï¼š
- [ ] å·²å®‰è£ Python 3.7+
- [ ] å·²å®‰è£ä¾è³´å¥—ä»¶ï¼ˆrequests, beautifulsoup4ï¼‰
- [ ] ç¶²è·¯é€£ç·šæ­£å¸¸
- [ ] æœ‰è¶³å¤ çš„ç£ç¢Ÿç©ºé–“

ä½¿ç”¨å¾Œæª¢æŸ¥ï¼š
- [ ] æª¢æŸ¥ `docs/source_texts/` æ˜¯å¦æœ‰æ–‡å­—æª”æ¡ˆ
- [ ] æª¢æŸ¥ `data/crawled/` æ˜¯å¦æœ‰ JSON æª”æ¡ˆ
- [ ] æŸ¥çœ‹æ—¥èªŒæª”æ¡ˆç¢ºèªæ²’æœ‰éŒ¯èª¤
- [ ] é©—è­‰å…§å®¹æ˜¯å¦å®Œæ•´

## ğŸ‰ é–‹å§‹ä½¿ç”¨ï¼

ç¾åœ¨ä½ å·²ç¶“æº–å‚™å¥½äº†ï¼Œé–‹å§‹çˆ¬å–ä½ çš„ç¬¬ä¸€æœ¬é“æ•™ç¶“å…¸å§ï¼

```bash
python crawler/shidian_crawler.py
```
