#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¶²é æ·±åº¦åˆ†æå·¥å…·

å­¸ç¿’é‡é»ï¼š
1. å¦‚ä½•è©³ç´°åˆ†æç¶²é çµæ§‹
2. å¦‚ä½•è­˜åˆ¥å‹•æ…‹è¼‰å…¥å…§å®¹
3. å¦‚ä½•æ‰¾åˆ°éš±è—çš„å…§å®¹é¸æ“‡å™¨
4. å¦‚ä½•è™•ç†è¤‡é›œçš„ç¶²ç«™æ¶æ§‹
"""

import json
from base_crawler import BaseCrawler
from core.unicode_handler import safe_print

class PageAnalyzer(BaseCrawler):
    """ç¶²é æ·±åº¦åˆ†æå·¥å…·"""
    
    def __init__(self):
        super().__init__()
        
    def deep_analyze(self, url):
        """æ·±åº¦åˆ†æç¶²é çµæ§‹"""
        safe_print(f"ğŸ”¬ æ·±åº¦åˆ†æç¶²é : {url}")
        safe_print("=" * 60)
        
        response = self.make_request(url)
        if not response:
            return None
            
        soup = self.parse_html(response.text)
        
        # 1. åŸºæœ¬è³‡è¨Š
        safe_print("ğŸ“‹ åŸºæœ¬è³‡è¨Š:")
        safe_print(f"ç‹€æ…‹ç¢¼: {response.status_code}")
        safe_print(f"å…§å®¹é¡å‹: {response.headers.get('content-type', 'unknown')}")
        safe_print(f"é é¢å¤§å°: {len(response.text)} å­—ç¬¦")
        safe_print()
        
        # 2. æ¨™é¡Œåˆ†æ
        safe_print("ğŸ“ æ¨™é¡Œåˆ†æ:")
        title = soup.find('title')
        if title:
            safe_print(f"é é¢æ¨™é¡Œ: {title.get_text().strip()}")
        
        for i in range(1, 7):
            headers = soup.find_all(f'h{i}')
            if headers:
                safe_print(f"H{i} æ¨™é¡Œ ({len(headers)}å€‹):")
                for h in headers[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    safe_print(f"  - {h.get_text().strip()}")
        safe_print()
        
        # 3. å…§å®¹å®¹å™¨åˆ†æ
        safe_print("ğŸ“¦ å…§å®¹å®¹å™¨åˆ†æ:")
        containers = [
            ('div', 'DIVå…ƒç´ '),
            ('article', 'ARTICLEå…ƒç´ '),
            ('section', 'SECTIONå…ƒç´ '),
            ('main', 'MAINå…ƒç´ '),
            ('p', 'æ®µè½å…ƒç´ ')
        ]
        
        for tag, desc in containers:
            elements = soup.find_all(tag)
            if elements:
                safe_print(f"{desc}: {len(elements)}å€‹")
                # æ‰¾å‡ºæœ€é•·çš„å…§å®¹
                longest = max(elements, key=lambda x: len(x.get_text()))
                text = longest.get_text().strip()
                if len(text) > 50:
                    safe_print(f"  æœ€é•·å…§å®¹: {len(text)} å­—ç¬¦")
                    safe_print(f"  é è¦½: {text[:100]}...")
                    # é¡¯ç¤ºé€™å€‹å…ƒç´ çš„classå’Œid
                    if longest.get('class'):
                        safe_print(f"  Class: {' '.join(longest.get('class'))}")
                    if longest.get('id'):
                        safe_print(f"  ID: {longest.get('id')}")
        safe_print()
        
        # 4. JavaScriptåˆ†æ
        safe_print("ğŸ”§ JavaScriptåˆ†æ:")
        scripts = soup.find_all('script')
        safe_print(f"è…³æœ¬æ•¸é‡: {len(scripts)}")
        
        js_keywords = ['ajax', 'fetch', 'xhr', 'load', 'content', 'text']
        for script in scripts:
            if script.string:
                script_text = script.string.lower()
                found_keywords = [kw for kw in js_keywords if kw in script_text]
                if found_keywords:
                    safe_print(f"  ç™¼ç¾é—œéµå­—: {', '.join(found_keywords)}")
                    if 'src' in script.attrs:
                        safe_print(f"  å¤–éƒ¨è…³æœ¬: {script['src']}")
        safe_print()
        
        # 5. CSSé¡åˆ¥åˆ†æ
        safe_print("ğŸ¨ CSSé¡åˆ¥åˆ†æ:")
        all_classes = set()
        for elem in soup.find_all(class_=True):
            all_classes.update(elem.get('class'))
        
        # å°‹æ‰¾å¯èƒ½çš„å…§å®¹ç›¸é—œé¡åˆ¥
        content_related = [cls for cls in all_classes if any(
            keyword in cls.lower() for keyword in 
            ['content', 'text', 'article', 'chapter', 'book', 'page']
        )]
        
        if content_related:
            safe_print("å¯èƒ½çš„å…§å®¹ç›¸é—œé¡åˆ¥:")
            for cls in sorted(content_related)[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                elements = soup.find_all(class_=cls)
                if elements:
                    max_text_len = max(len(elem.get_text()) for elem in elements)
                    safe_print(f"  .{cls}: {len(elements)}å€‹å…ƒç´ , æœ€å¤§å…§å®¹é•·åº¦: {max_text_len}")
        safe_print()
        
        # 6. è¡¨å–®åˆ†æ
        safe_print("ğŸ“‹ è¡¨å–®åˆ†æ:")
        forms = soup.find_all('form')
        if forms:
            safe_print(f"è¡¨å–®æ•¸é‡: {len(forms)}")
            for i, form in enumerate(forms):
                action = form.get('action', 'ç„¡')
                method = form.get('method', 'GET')
                safe_print(f"  è¡¨å–®{i+1}: {method} -> {action}")
        else:
            safe_print("æœªç™¼ç¾è¡¨å–®")
        safe_print()
        
        # 7. ç‰¹æ®Šå…ƒç´ åˆ†æ
        safe_print("ğŸ” ç‰¹æ®Šå…ƒç´ åˆ†æ:")
        
        # iframeåµŒå…¥
        iframes = soup.find_all('iframe')
        if iframes:
            safe_print(f"iframeåµŒå…¥: {len(iframes)}å€‹")
            
        # æ•¸æ“šå±¬æ€§å…ƒç´ 
        data_elements = soup.find_all(attrs=lambda x: x and any(k.startswith('data-') for k in x.keys()))
        if data_elements:
            safe_print(f"æ•¸æ“šå±¬æ€§å…ƒç´ : {len(data_elements)}å€‹")
            
        # æ‡¶è¼‰å…¥å…ƒç´ 
        lazy_elements = soup.find_all(class_='lazy')
        if lazy_elements:
            safe_print(f"æ‡¶è¼‰å…¥å…ƒç´ : {len(lazy_elements)}å€‹")
            
        # éš±è—å…ƒç´ 
        hidden_elements = soup.find_all(attrs={'style': lambda x: x and 'display:none' in x})
        if hidden_elements:
            safe_print(f"éš±è—å…ƒç´ : {len(hidden_elements)}å€‹")
        
        return {
            'url': url,
            'status_code': response.status_code,
            'content_length': len(response.text),
            'title': title.get_text().strip() if title else None,
            'content_classes': content_related,
            'script_count': len(scripts)
        }

def analyze_shidian_page():
    """åˆ†æåå…¸å¤ç±é é¢"""
    analyzer = PageAnalyzer()
    
    url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1?page_from=home_page&version=19"
    
    result = analyzer.deep_analyze(url)
    
    safe_print("ğŸ¯ åˆ†æå»ºè­°:")
    safe_print("1. æª¢æŸ¥æ˜¯å¦æœ‰å‹•æ…‹è¼‰å…¥çš„å…§å®¹")
    safe_print("2. å˜—è©¦ä¸åŒçš„å…§å®¹é¸æ“‡å™¨")
    safe_print("3. å¯èƒ½éœ€è¦æ¨¡æ“¬ç€è¦½å™¨è¡Œç‚º")
    safe_print("4. æª¢æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šçš„è«‹æ±‚æ¨™é ­")

if __name__ == "__main__":
    analyze_shidian_page()