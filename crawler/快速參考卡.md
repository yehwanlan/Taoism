# ğŸš€ çˆ¬èŸ²å·¥å…·å¿«é€Ÿåƒè€ƒå¡

## ğŸ¯ ä¸€åˆ†é˜å¿«é€Ÿé¸æ“‡

### æˆ‘æƒ³è¦...

| éœ€æ±‚ | ä½¿ç”¨é€™å€‹ | å‘½ä»¤ |
|------|---------|------|
| çˆ¬å–åå…¸å¤ç±ç¶² | `shidian_simple.py` | `python shidian_simple.py` |
| åˆ†ææ–°ç¶²ç«™ | `page_analyzer.py` | `python page_analyzer.py` |
| æ‰¹é‡è™•ç†æŠ±æœ´å­ | `baopuzi_crawler.py` | `python baopuzi_crawler.py` |
| å­¸ç¿’çˆ¬èŸ²åŸºç¤ | `demo.py` | `python demo.py` |
| è™•ç†å‹•æ…‹ç¶²ç«™ | `selenium_crawler.py` | éœ€è¦å®‰è£Chrome |

## ğŸ”§ å¸¸ç”¨ä»£ç¢¼ç‰‡æ®µ

### åŸºç¤çˆ¬å–
```python
from shidian_simple import ShidianSimple

crawler = ShidianSimple()
success = crawler.crawl("ç¶²å€", "æª”æ¡ˆå")
```

### ç¶²ç«™åˆ†æ
```python
from page_analyzer import PageAnalyzer

analyzer = PageAnalyzer()
result = analyzer.deep_analyze("ç¶²å€")
```

### æ‰¹é‡è™•ç†
```python
from baopuzi_crawler import BaopuziCrawler

crawler = BaopuziCrawler()
chapters = [{'number': 1, 'title': 'æ¨™é¡Œ', 'url': 'ç¶²å€'}]
for chapter in chapters:
    crawler.crawl_chapter(chapter['url'], chapter['number'])
```

## âš¡ ç·Šæ€¥æ•…éšœæ’é™¤

| å•é¡Œ | è§£æ±ºæ–¹æ¡ˆ |
|------|---------|
| æ¨¡çµ„æ‰¾ä¸åˆ° | `pip install -r requirements.txt` |
| ç·¨ç¢¼éŒ¯èª¤ | æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ä½¿ç”¨UTF-8ç·¨ç¢¼ |
| ç¶²è·¯è¶…æ™‚ | å¢åŠ å»¶é²æ™‚é–“ï¼Œæª¢æŸ¥ç¶²è·¯é€£æ¥ |
| å…§å®¹ç‚ºç©º | ä½¿ç”¨ `page_analyzer.py` é‡æ–°åˆ†æç¶²ç«™ |
| SeleniuméŒ¯èª¤ | æª¢æŸ¥Chromeå’ŒChromeDriverç‰ˆæœ¬ |

## ğŸ“Š å·¥å…·é¸æ“‡æ±ºç­–æ¨¹

```
é–‹å§‹
â”œâ”€â”€ æ˜¯åå…¸å¤ç±ç¶²ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ shidian_simple.py âœ…
â”‚   â””â”€â”€ å¦ â†“
â”œâ”€â”€ æ˜¯æ–°ç¶²ç«™ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ page_analyzer.py â†’ æ ¹æ“šçµæœé¸æ“‡
â”‚   â””â”€â”€ å¦ â†“
â”œâ”€â”€ éœ€è¦è™•ç†JavaScriptï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ selenium_crawler.py
â”‚   â””â”€â”€ å¦ â†“
â”œâ”€â”€ æ˜¯APIç¶²ç«™ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ api_crawler.py
â”‚   â””â”€â”€ å¦ â†’ taoism_crawler.py
```

## ğŸ® å‘½ä»¤åˆ—å¿«é€Ÿåƒè€ƒ

```bash
# åŸºç¤ä½¿ç”¨
python shidian_simple.py                    # çˆ¬å–é è¨­URL
python page_analyzer.py                     # åˆ†æé è¨­URL
python demo.py                             # é‹è¡Œç¤ºä¾‹

# é€²éšä½¿ç”¨
python run_crawler.py --help               # æŸ¥çœ‹æ‰€æœ‰é¸é …
python run_crawler.py --mode validate      # é©—è­‰URL
python run_crawler.py --mode crawl         # é–‹å§‹çˆ¬å–

# å®‰è£å’Œæª¢æŸ¥
pip install -r requirements.txt            # å®‰è£ä¾è³´
python -c "import requests; print('OK')"   # æª¢æŸ¥å®‰è£
```

## ğŸ” èª¿è©¦æª¢æŸ¥æ¸…å–®

- [ ] æª¢æŸ¥ `crawler.log` æ—¥èªŒæª”æ¡ˆ
- [ ] æŸ¥çœ‹ `debug_*.txt` èª¿è©¦æª”æ¡ˆ
- [ ] ç¢ºèªç¶²è·¯é€£æ¥æ­£å¸¸
- [ ] é©—è­‰ç›®æ¨™ç¶²ç«™å¯è¨ªå•
- [ ] æª¢æŸ¥Pythonå¥—ä»¶ç‰ˆæœ¬
- [ ] ç¢ºèªæª”æ¡ˆæ¬Šé™æ­£ç¢º

## ğŸ“± è¯çµ¡è³‡è¨Š

- ğŸ“ å°ˆæ¡ˆä½ç½®ï¼š`C:\Users\ilove\Desktop\Taoism\Taoism\crawler`
- ğŸ“‹ ä¸»è¦æ–‡æª”ï¼š`çˆ¬èŸ²å·¥å…·èªªæ˜æ–‡æª”.md`
- ğŸ“Š åŠŸèƒ½å°ç…§ï¼š`å·¥å…·åŠŸèƒ½å°ç…§è¡¨.md`
- ğŸ”§ å¯¦ç”¨æŒ‡å—ï¼š`practical_guide.md`

---
*æœ€å¾Œæ›´æ–°ï¼š2025-08-09*
*ç‰ˆæœ¬ï¼šv2.0*