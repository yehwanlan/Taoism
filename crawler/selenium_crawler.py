#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seleniumå‹•æ…‹ç¶²é çˆ¬èŸ²

å­¸ç¿’é‡é»ï¼š
1. å¦‚ä½•ä½¿ç”¨Seleniumè™•ç†å‹•æ…‹è¼‰å…¥çš„å…§å®¹
2. å¦‚ä½•ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
3. å¦‚ä½•æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨è¡Œç‚º
4. å¦‚ä½•è™•ç†JavaScriptæ¸²æŸ“çš„å…§å®¹

æ³¨æ„ï¼šéœ€è¦å…ˆå®‰è£ selenium å’Œç€è¦½å™¨é©…å‹•
pip install selenium
"""

import time
import os
from pathlib import Path

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸  Seleniumæœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install selenium")

from base_crawler import BaseCrawler

class SeleniumCrawler(BaseCrawler):
    """ä½¿ç”¨Seleniumçš„å‹•æ…‹ç¶²é çˆ¬èŸ²"""
    
    def __init__(self, headless=True):
        super().__init__()
        self.headless = headless
        self.driver = None
        
        if not SELENIUM_AVAILABLE:
            raise ImportError("Seleniumæœªå®‰è£ï¼Œè«‹å…ˆå®‰è£: pip install selenium")
            
    def setup_driver(self):
        """è¨­å®šç€è¦½å™¨é©…å‹•"""
        print("ğŸ”§ è¨­å®šç€è¦½å™¨é©…å‹•...")
        
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')  # ç„¡é ­æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # è¨­å®šç”¨æˆ¶ä»£ç†
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)  # éš±å¼ç­‰å¾…
            print("âœ… Chromeé©…å‹•è¨­å®šæˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Chromeé©…å‹•è¨­å®šå¤±æ•—: {e}")
            print("ğŸ’¡ è«‹ç¢ºä¿å·²å®‰è£ChromeDriveræˆ–ä½¿ç”¨webdriver-manager")
            return False
            
    def wait_for_content(self, timeout=15):
        """ç­‰å¾…é é¢å…§å®¹è¼‰å…¥"""
        print("â³ ç­‰å¾…é é¢è¼‰å…¥...")
        
        # ç­‰å¾…é é¢åŸºæœ¬è¼‰å…¥å®Œæˆ
        WebDriverWait(self.driver, timeout).until(
            lambda driver: driver.execute_script("return document.readyState") == "complete"
        )
        
        # é¡å¤–ç­‰å¾…JavaScriptåŸ·è¡Œ
        time.sleep(3)
        
        print("âœ… é é¢è¼‰å…¥å®Œæˆ")
        
    def extract_dynamic_content(self, url):
        """æå–å‹•æ…‹è¼‰å…¥çš„å…§å®¹"""
        print(f"ğŸŒ é–‹å§‹è¼‰å…¥å‹•æ…‹é é¢: {url}")
        
        if not self.driver:
            if not self.setup_driver():
                return None
                
        try:
            # è¼‰å…¥é é¢
            self.driver.get(url)
            self.wait_for_content()
            
            # å˜—è©¦å¤šç¨®å…§å®¹é¸æ“‡å™¨
            content_selectors = [
                "//div[contains(@class, 'content')]",
                "//div[contains(@class, 'text')]", 
                "//div[contains(@class, 'chapter')]",
                "//article",
                "//main",
                "//div[contains(@class, 'book')]",
                "//p[string-length(text()) > 50]"  # é•·åº¦è¶…é50çš„æ®µè½
            ]
            
            content_found = False
            all_content = []
            
            for selector in content_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    if elements:
                        print(f"âœ… æ‰¾åˆ°å…§å®¹ (é¸æ“‡å™¨: {selector}, å…ƒç´ æ•¸: {len(elements)})")
                        for elem in elements:
                            text = elem.text.strip()
                            if len(text) > 50:  # åªæ”¶é›†æœ‰æ„ç¾©çš„å…§å®¹
                                all_content.append(text)
                                content_found = True
                except Exception as e:
                    continue
                    
            if content_found:
                # å»é‡ä¸¦åˆä½µå…§å®¹
                unique_content = []
                seen = set()
                for content in all_content:
                    if content not in seen and len(content) > 20:
                        unique_content.append(content)
                        seen.add(content)
                        
                final_content = '\n\n'.join(unique_content)
                print(f"ğŸ“ æå–åˆ°å…§å®¹ï¼Œç¸½é•·åº¦: {len(final_content)} å­—ç¬¦")
                return final_content
            else:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆå…§å®¹")
                return None
                
        except TimeoutException:
            print("â° é é¢è¼‰å…¥è¶…æ™‚")
            return None
        except Exception as e:
            print(f"âŒ æå–å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
            
    def crawl_dynamic_page(self, url, title=None):
        """çˆ¬å–å‹•æ…‹é é¢"""
        print(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å–å‹•æ…‹é é¢")
        print(f"ç¶²å€: {url}")
        print("-" * 50)
        
        content = self.extract_dynamic_content(url)
        
        if not content:
            return False
            
        # ç¢ºå®šæ¨™é¡Œ
        if not title:
            try:
                page_title = self.driver.title
                title = page_title.split('-')[0].strip() if '-' in page_title else page_title
            except:
                title = "å‹•æ…‹çˆ¬å–_æœªçŸ¥æ¨™é¡Œ"
                
        # æ¸…ç†å…§å®¹
        cleaned_content = self.clean_text(content)
        
        # å„²å­˜å…§å®¹
        filename = f"{title}.txt"
        self.save_text(cleaned_content, filename, "../docs/source_texts")
        
        print(f"âœ… æˆåŠŸçˆ¬å–: {title}")
        print(f"å…§å®¹é•·åº¦: {len(cleaned_content)} å­—ç¬¦")
        
        return True
        
    def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ ç€è¦½å™¨å·²é—œé–‰")

# ä½¿ç”¨ç¤ºä¾‹
def crawl_shidian_with_selenium():
    """ä½¿ç”¨Seleniumçˆ¬å–åå…¸å¤ç±"""
    
    if not SELENIUM_AVAILABLE:
        print("âŒ Seleniumæœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨å‹•æ…‹çˆ¬èŸ²")
        print("è«‹åŸ·è¡Œ: pip install selenium")
        print("ä¸¦ä¸‹è¼‰ChromeDriver: https://chromedriver.chromium.org/")
        return
        
    crawler = SeleniumCrawler(headless=True)  # è¨­ç‚ºFalseå¯ä»¥çœ‹åˆ°ç€è¦½å™¨æ“ä½œ
    
    try:
        url = "https://www.shidianguji.com/book/SBCK109/chapter/1j70ybwytkcak_1?page_from=home_page&version=19"
        
        success = crawler.crawl_dynamic_page(url, "æŠ±æœ´å­_ç¬¬ä¸€ç« _åå…¸å¤ç±")
        
        if success:
            print("\nğŸ‰ å‹•æ…‹çˆ¬å–æˆåŠŸï¼")
        else:
            print("\nâŒ å‹•æ…‹çˆ¬å–å¤±æ•—")
            
    finally:
        crawler.close()

if __name__ == "__main__":
    crawl_shidian_with_selenium()