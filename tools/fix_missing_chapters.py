#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
from core.unicode_handler import safe_print
ä¿®å¾©ç¼ºå¤±ç« ç¯€å·¥å…·

å°ˆé–€ç”¨æ–¼ä¿®å¾© å¤ªä¸Šæ´ç„çµå®ä¸šæŠ¥å› ç¼˜ç»_DZ0336 ç­‰ç¶“å…¸ä¸­ç¼ºå¤±çš„å“ï¼ˆç« ç¯€ï¼‰
"""

import requests
import re
import json
import time
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin


class ChapterFixer:
    """ç« ç¯€ä¿®å¾©å™¨"""
    
    def __init__(self):
        self.base_url = "https://www.shidianguji.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def analyze_book_structure(self, book_id):
        """åˆ†ææ›¸ç±çµæ§‹ï¼Œæ‰¾å‡ºæ‰€æœ‰ç« ç¯€"""
        safe_print(f"ğŸ” åˆ†ææ›¸ç±çµæ§‹: {book_id}")
        
        book_url = f"{self.base_url}/book/{book_id}"
        
        try:
            response = self.session.get(book_url, timeout=10)
            response.raise_for_status()
            
            # ä¿å­˜åŸå§‹HTMLç”¨æ–¼èª¿è©¦
            debug_file = Path(f"debug_{book_id}_structure.html")
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            safe_print(f"ğŸ’¾ çµæ§‹èª¿è©¦æ–‡ä»¶: {debug_file}")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾ç« ç¯€åˆ—è¡¨
            chapters = []
            
            # æ–¹æ³•1: å°‹æ‰¾ç« ç¯€éˆæ¥
            chapter_links = soup.find_all('a', href=re.compile(r'/chapter/'))
            for link in chapter_links:
                href = link.get('href')
                title = link.get_text().strip()
                if href and title and len(title) > 2:
                    chapters.append({
                        'title': title,
                        'url': urljoin(self.base_url, href),
                        'chapter_id': self._extract_chapter_id(href)
                    })
            
            # æ–¹æ³•2: å¾JavaScriptæ•¸æ“šä¸­æå–
            script_chapters = self._extract_chapters_from_scripts(response.text)
            if script_chapters:
                chapters.extend(script_chapters)
            
            # å»é‡
            seen_ids = set()
            unique_chapters = []
            for chapter in chapters:
                if chapter['chapter_id'] not in seen_ids:
                    unique_chapters.append(chapter)
                    seen_ids.add(chapter['chapter_id'])
            
            safe_print(f"âœ… æ‰¾åˆ° {len(unique_chapters)} å€‹ç« ç¯€")
            for i, chapter in enumerate(unique_chapters, 1):
                safe_print(f"  {i}. {chapter['title']} ({chapter['chapter_id']})")
            
            return unique_chapters
            
        except Exception as e:
            safe_print(f"âŒ åˆ†ææ›¸ç±çµæ§‹å¤±æ•—: {e}")
            return []
    
    def _extract_chapter_id(self, href):
        """å¾hrefæå–ç« ç¯€ID"""
        match = re.search(r'/chapter/([^/?]+)', href)
        return match.group(1) if match else None
    
    def _extract_chapters_from_scripts(self, html_content):
        """å¾JavaScriptæ•¸æ“šä¸­æå–ç« ç¯€ä¿¡æ¯"""
        chapters = []
        
        # å°‹æ‰¾åŒ…å«ç« ç¯€æ•¸æ“šçš„JavaScript
        patterns = [
            r'window\._ROUTER_DATA\s*=\s*({.*?});',
            r'window\._SSR_DATA\s*=\s*({.*?});',
            r'"chapters":\s*(\[.*?\])',
            r'"chapterList":\s*(\[.*?\])',
        ]
        
        for pattern in patterns:
            matches = re.search(pattern, html_content, re.DOTALL)
            if matches:
                try:
                    data_str = matches.group(1)
                    if data_str.startswith('['):
                        # ç›´æ¥æ˜¯ç« ç¯€æ•¸çµ„
                        chapter_data = json.loads(data_str)
                    else:
                        # æ˜¯å®Œæ•´çš„æ•¸æ“šå°è±¡
                        full_data = json.loads(data_str)
                        chapter_data = self._find_chapters_in_data(full_data)
                    
                    if chapter_data:
                        for item in chapter_data:
                            if isinstance(item, dict):
                                title = item.get('title') or item.get('name') or item.get('chapterName')
                                chapter_id = item.get('id') or item.get('chapterId')
                                if title and chapter_id:
                                    chapters.append({
                                        'title': title,
                                        'url': f"{self.base_url}/book/{book_id}/chapter/{chapter_id}",
                                        'chapter_id': chapter_id
                                    })
                        break
                        
                except json.JSONDecodeError:
                    continue
        
        return chapters
    
    def _find_chapters_in_data(self, data):
        """åœ¨æ•¸æ“šçµæ§‹ä¸­éæ­¸å°‹æ‰¾ç« ç¯€ä¿¡æ¯"""
        if isinstance(data, dict):
            # æª¢æŸ¥å¸¸è¦‹çš„ç« ç¯€å­—æ®µå
            for key in ['chapters', 'chapterList', 'contents', 'items']:
                if key in data and isinstance(data[key], list):
                    return data[key]
            
            # éæ­¸æœç´¢
            for value in data.values():
                result = self._find_chapters_in_data(value)
                if result:
                    return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._find_chapters_in_data(item)
                if result:
                    return result
        
        return None
    
    def crawl_missing_chapter(self, book_id, chapter_info):
        """çˆ¬å–ç¼ºå¤±çš„ç« ç¯€"""
        safe_print(f"ğŸ“– çˆ¬å–ç« ç¯€: {chapter_info['title']}")
        
        try:
            # å˜—è©¦å¤šç¨®APIç«¯é»
            api_urls = [
                f"{self.base_url}/api/book/{book_id}/chapter/{chapter_info['chapter_id']}",
                f"{self.base_url}/api/ancientlib/volume/{chapter_info['chapter_id']}/content",
                chapter_info['url']
            ]
            
            content = None
            for api_url in api_urls:
                safe_print(f"  å˜—è©¦: {api_url}")
                content = self._try_extract_content(api_url)
                if content:
                    break
                time.sleep(1)  # é¿å…è«‹æ±‚éå¿«
            
            if content:
                return {
                    'title': chapter_info['title'],
                    'content': content,
                    'chapter_id': chapter_info['chapter_id']
                }
            else:
                safe_print(f"âŒ ç„¡æ³•ç²å–ç« ç¯€å…§å®¹: {chapter_info['title']}")
                return None
                
        except Exception as e:
            safe_print(f"âŒ çˆ¬å–ç« ç¯€å¤±æ•—: {e}")
            return None
    
    def _try_extract_content(self, url):
        """å˜—è©¦å¾URLæå–å…§å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # å¦‚æœæ˜¯JSONéŸ¿æ‡‰
            if 'application/json' in response.headers.get('content-type', ''):
                data = response.json()
                return self._extract_text_from_json(data)
            
            # å¦‚æœæ˜¯HTMLéŸ¿æ‡‰
            else:
                soup = BeautifulSoup(response.text, 'html.parser')
                return self._extract_text_from_html(soup)
                
        except Exception:
            return None
    
    def _extract_text_from_json(self, data):
        """å¾JSONæ•¸æ“šä¸­æå–æ–‡æœ¬"""
        content_parts = []
        
        def extract_recursive(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key in ['content', 'text', 'body'] and isinstance(value, str):
                        text = value.strip()
                        if len(text) > 10 and re.search(r'[\u4e00-\u9fff]', text):
                            content_parts.append(text)
                    else:
                        extract_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_recursive(item)
        
        extract_recursive(data)
        
        if content_parts:
            return '\n\n'.join(content_parts)
        return None
    
    def _extract_text_from_html(self, soup):
        """å¾HTMLä¸­æå–æ–‡æœ¬"""
        # å°‹æ‰¾ä¸»è¦å…§å®¹å€åŸŸ
        content_selectors = [
            'main.read-layout-main article.chapter-reader',
            '.chapter-content',
            '.book-content',
            '.text-content',
            'article',
            '.main-content'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content_parts = []
                for element in elements[0].find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):
                    text = element.get_text().strip()
                    if text and len(text) > 3:
                        content_parts.append(text)
                
                if content_parts:
                    return '\n\n'.join(content_parts)
        
        return None
    
    def save_chapter(self, book_folder, chapter_data, chapter_number):
        """ä¿å­˜ç« ç¯€åˆ°æ–‡ä»¶"""
        # æ¸…ç†æ–‡ä»¶å
        clean_title = re.sub(r'[<>:"/\\|?*]', '_', chapter_data['title'])
        filename = f"{chapter_number:02d}_{clean_title}.txt"
        
        file_path = book_folder / "åŸæ–‡" / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {chapter_data['title']}\n\n")
            f.write(chapter_data['content'])
        
        safe_print(f"âœ… å·²ä¿å­˜: {filename}")
        return filename
    
    def fix_missing_chapters(self, book_id, book_folder_name):
        """ä¿®å¾©ç¼ºå¤±çš„ç« ç¯€"""
        safe_print(f"ğŸ”§ é–‹å§‹ä¿®å¾©ç¼ºå¤±ç« ç¯€: {book_id}")
        safe_print("=" * 60)
        
        # 1. åˆ†ææ›¸ç±çµæ§‹
        all_chapters = self.analyze_book_structure(book_id)
        if not all_chapters:
            safe_print("âŒ ç„¡æ³•ç²å–ç« ç¯€åˆ—è¡¨")
            return False
        
        # 2. æª¢æŸ¥ç¾æœ‰æ–‡ä»¶
        book_folder = Path(f"../docs/source_texts/{book_folder_name}")
        source_folder = book_folder / "åŸæ–‡"
        
        if not source_folder.exists():
            safe_print(f"âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶å¤¾: {source_folder}")
            return False
        
        existing_files = list(source_folder.glob("*.txt"))
        existing_titles = set()
        
        for file_path in existing_files:
            # å¾æ–‡ä»¶åæå–æ¨™é¡Œ
            filename = file_path.stem
            # ç§»é™¤ç·¨è™Ÿå‰ç¶´
            title_part = re.sub(r'^\d+_', '', filename)
            existing_titles.add(title_part)
        
        safe_print(f"ğŸ“ ç¾æœ‰æ–‡ä»¶: {len(existing_files)} å€‹")
        
        # 3. æ‰¾å‡ºç¼ºå¤±çš„ç« ç¯€
        missing_chapters = []
        for chapter in all_chapters:
            clean_title = re.sub(r'[<>:"/\\|?*]', '_', chapter['title'])
            if clean_title not in existing_titles:
                missing_chapters.append(chapter)
        
        safe_print(f"âŒ ç¼ºå¤±ç« ç¯€: {len(missing_chapters)} å€‹")
        for chapter in missing_chapters:
            safe_print(f"  - {chapter['title']}")
        
        if not missing_chapters:
            safe_print("âœ… æ²’æœ‰ç¼ºå¤±çš„ç« ç¯€")
            return True
        
        # 4. çˆ¬å–ç¼ºå¤±çš„ç« ç¯€
        success_count = 0
        for i, chapter in enumerate(missing_chapters, 1):
            safe_print(f"\nğŸ“– è™•ç†ç¼ºå¤±ç« ç¯€ {i}/{len(missing_chapters)}")
            
            chapter_data = self.crawl_missing_chapter(book_id, chapter)
            if chapter_data:
                # ç¢ºå®šç« ç¯€ç·¨è™Ÿ
                chapter_number = len(existing_files) + success_count + 1
                
                # ä¿å­˜ç« ç¯€
                filename = self.save_chapter(book_folder, chapter_data, chapter_number)
                success_count += 1
                
                # æ›´æ–°README
                self._update_readme(book_folder, chapter_data['title'], chapter_number)
            
            time.sleep(2)  # é¿å…è«‹æ±‚éå¿«
        
        safe_print(f"\nğŸ‰ ä¿®å¾©å®Œæˆï¼æˆåŠŸæ·»åŠ  {success_count}/{len(missing_chapters)} å€‹ç« ç¯€")
        return success_count > 0
    
    def _update_readme(self, book_folder, chapter_title, chapter_number):
        """æ›´æ–°READMEæ–‡ä»¶"""
        readme_path = book_folder / "README.md"
        if readme_path.exists():
            try:
                with open(readme_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åœ¨ç« ç¯€åˆ—è¡¨ä¸­æ·»åŠ æ–°ç« ç¯€
                new_line = f"- {chapter_number:02d}. {chapter_title}"
                
                # æ‰¾åˆ°ç« ç¯€åˆ—è¡¨éƒ¨åˆ†ä¸¦æ·»åŠ 
                if "## ç« ç¯€åˆ—è¡¨" in content:
                    lines = content.split('\n')
                    for i, line in enumerate(lines):
                        if line.startswith("## ç« ç¯€åˆ—è¡¨"):
                            # æ‰¾åˆ°åˆ—è¡¨çµæŸä½ç½®
                            j = i + 1
                            while j < len(lines) and (lines[j].startswith('- ') or lines[j].strip() == ''):
                                j += 1
                            # æ’å…¥æ–°ç« ç¯€
                            lines.insert(j, new_line)
                            break
                    
                    with open(readme_path, 'w', encoding='utf-8') as f:
                        f.write('\n'.join(lines))
                
            except Exception as e:
                safe_print(f"âš ï¸  æ›´æ–°READMEå¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    fixer = ChapterFixer()
    
    # ä¿®å¾© å¤ªä¸Šæ´ç„çµå®ä¸šæŠ¥å› ç¼˜ç»_DZ0336
    book_id = "DZ0336"
    book_folder_name = "å¤ªä¸Šæ´ç„çµå®ä¸šæŠ¥å› ç¼˜ç»_DZ0336"
    
    success = fixer.fix_missing_chapters(book_id, book_folder_name)
    
    if success:
        safe_print("\nğŸŠ ä¿®å¾©æˆåŠŸï¼")
        safe_print("ğŸ’¡ å»ºè­°ï¼šæª¢æŸ¥æ–°æ·»åŠ çš„ç« ç¯€å…§å®¹æ˜¯å¦æ­£ç¢º")
    else:
        safe_print("\nâŒ ä¿®å¾©å¤±æ•—")
        safe_print("ğŸ’¡ å»ºè­°ï¼šæª¢æŸ¥ç¶²çµ¡é€£æ¥å’Œèª¿è©¦æ–‡ä»¶")


if __name__ == "__main__":
    main()