#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIç¿»è­¯å“è³ªè©•ä¼°å·¥å…·

ç”¨æ–¼è©•ä¼°AIç¿»è­¯çš„å“è³ªå’Œè¦ç¯„ç¬¦åˆåº¦
"""

import re
import json
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime


class TranslationEvaluator:
    """ç¿»è­¯å“è³ªè©•ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è©•ä¼°å™¨"""
        self.load_terminology()
        self.load_evaluation_rules()
        
    def load_terminology(self):
        """è¼‰å…¥è¡“èªå°ç…§è¡¨"""
        self.terminology = {
            # é“æ•™ä¸‰æ¸…
            "å…ƒå§‹å¤©å°Š": {"category": "ä¸‰æ¸…", "keep_original": True},
            "éˆå¯¶å¤©å°Š": {"category": "ä¸‰æ¸…", "keep_original": True},
            "é“å¾·å¤©å°Š": {"category": "ä¸‰æ¸…", "keep_original": True},
            "å¤ªä¸Šè€å›": {"category": "ä¸‰æ¸…", "keep_original": True},
            
            # é‡è¦æ¦‚å¿µ
            "å¤ªä¸Š": {"category": "å°Šç¨±", "keep_original": True},
            "ç„¡ç‚º": {"category": "å“²å­¸", "keep_original": True},
            "è‡ªç„¶": {"category": "å“²å­¸", "keep_original": True},
            "é“": {"category": "æ ¸å¿ƒæ¦‚å¿µ", "keep_original": True},
            "å¾·": {"category": "æ ¸å¿ƒæ¦‚å¿µ", "keep_original": True},
            
            # ä¿®ç…‰è¡“èª
            "ä¿®çœŸ": {"category": "ä¿®ç…‰", "alternatives": ["ä¿®é“", "ä¿®ç…‰"]},
            "ç…‰ä¸¹": {"category": "ä¿®ç…‰", "keep_original": True},
            "æœæ°£": {"category": "ä¿®ç…‰", "alternatives": ["æœæ°£", "æ°£åŠŸä¿®ç…‰"]},
            "å°å¼•": {"category": "ä¿®ç…‰", "alternatives": ["å°å¼•", "é¤Šç”ŸåŠŸæ³•"]},
            
            # å®—æ•™è¡“èª
            "ç¬¦ç±™": {"category": "å®—æ•™", "alternatives": ["ç¬¦ç±™", "é“ç¬¦"]},
            "é½‹é†®": {"category": "å®—æ•™", "alternatives": ["é½‹é†®", "é“æ•™å„€å¼"]},
            "æ´å¤©ç¦åœ°": {"category": "å®—æ•™", "keep_original": True},
            
            # é¿å…ä½¿ç”¨çš„è©å½™
            "è¿·ä¿¡": {"category": "ç¦ç”¨", "reason": "å¸¶æœ‰è²¶ç¾©è‰²å½©"},
            "å°å»º": {"category": "ç¦ç”¨", "reason": "æ”¿æ²»è‰²å½©è©å½™"},
            "è€å­èªª": {"category": "ç¦ç”¨", "correct": "å¤ªä¸Šè€å›èªª"}
        }
        
    def load_evaluation_rules(self):
        """è¼‰å…¥è©•ä¼°è¦å‰‡"""
        self.rules = {
            "format_check": {
                "required_sections": ["åŸæ–‡", "ç¿»è­¯", "è¨»è§£"],
                "annotation_subsections": ["é‡è¦è©å½™", "æ–‡åŒ–èƒŒæ™¯", "ç¿»è­¯è¦é»"]
            },
            "content_check": {
                "min_translation_length": 10,
                "max_length_ratio": 3.0,  # ç¿»è­¯é•·åº¦ä¸æ‡‰è¶…éåŸæ–‡3å€
                "min_length_ratio": 0.3   # ç¿»è­¯é•·åº¦ä¸æ‡‰å°‘æ–¼åŸæ–‡30%
            },
            "terminology_check": {
                "check_consistency": True,
                "check_forbidden_terms": True,
                "check_preferred_terms": True
            }
        }
        
    def evaluate_translation(self, translation_text: str) -> Dict:
        """è©•ä¼°ç¿»è­¯å“è³ª"""
        results = {
            "overall_score": 0,
            "format_score": 0,
            "content_score": 0,
            "terminology_score": 0,
            "issues": [],
            "suggestions": []
        }
        
        # è§£æç¿»è­¯æ–‡æœ¬
        parsed = self.parse_translation(translation_text)
        
        # æ ¼å¼æª¢æŸ¥
        format_result = self.check_format(parsed)
        results["format_score"] = format_result["score"]
        results["issues"].extend(format_result["issues"])
        
        # å…§å®¹æª¢æŸ¥
        if parsed["original"] and parsed["translation"]:
            content_result = self.check_content(parsed["original"], parsed["translation"])
            results["content_score"] = content_result["score"]
            results["issues"].extend(content_result["issues"])
            
            # è¡“èªæª¢æŸ¥
            terminology_result = self.check_terminology(parsed["translation"])
            results["terminology_score"] = terminology_result["score"]
            results["issues"].extend(terminology_result["issues"])
            results["suggestions"].extend(terminology_result["suggestions"])
        
        # è¨ˆç®—ç¸½åˆ†
        results["overall_score"] = (
            results["format_score"] * 0.3 +
            results["content_score"] * 0.4 +
            results["terminology_score"] * 0.3
        )
        
        return results
        
    def parse_translation(self, text: str) -> Dict:
        """è§£æç¿»è­¯æ–‡æœ¬çµæ§‹"""
        sections = {
            "title": "",
            "original": "",
            "translation": "",
            "annotations": ""
        }
        
        # æå–æ¨™é¡Œ
        title_match = re.search(r'^#\s*(.+)', text, re.MULTILINE)
        if title_match:
            sections["title"] = title_match.group(1).strip()
            
        # æå–å„å€‹éƒ¨åˆ†
        original_match = re.search(r'##\s*åŸæ–‡\s*\n(.*?)(?=##|$)', text, re.DOTALL)
        if original_match:
            sections["original"] = original_match.group(1).strip()
            
        translation_match = re.search(r'##\s*ç¿»è­¯\s*\n(.*?)(?=##|$)', text, re.DOTALL)
        if translation_match:
            sections["translation"] = translation_match.group(1).strip()
            
        annotation_match = re.search(r'##\s*è¨»è§£\s*\n(.*?)(?=##|$)', text, re.DOTALL)
        if annotation_match:
            sections["annotations"] = annotation_match.group(1).strip()
            
        return sections
        
    def check_format(self, parsed: Dict) -> Dict:
        """æª¢æŸ¥æ ¼å¼è¦ç¯„"""
        issues = []
        score = 100
        
        # æª¢æŸ¥å¿…è¦éƒ¨åˆ†
        required_sections = self.rules["format_check"]["required_sections"]
        for section in required_sections:
            section_key = section.lower().replace("è¨»è§£", "annotations").replace("ç¿»è­¯", "translation").replace("åŸæ–‡", "original")
            if not parsed.get(section_key):
                issues.append(f"âŒ ç¼ºå°‘ã€Œ{section}ã€éƒ¨åˆ†")
                score -= 30
                
        # æª¢æŸ¥è¨»è§£å­éƒ¨åˆ†
        if parsed["annotations"]:
            annotation_subsections = self.rules["format_check"]["annotation_subsections"]
            for subsection in annotation_subsections:
                if subsection not in parsed["annotations"]:
                    issues.append(f"âš ï¸  è¨»è§£ä¸­ç¼ºå°‘ã€Œ{subsection}ã€å­éƒ¨åˆ†")
                    score -= 10
                    
        return {"score": max(0, score), "issues": issues}
        
    def check_content(self, original: str, translation: str) -> Dict:
        """æª¢æŸ¥å…§å®¹å“è³ª"""
        issues = []
        score = 100
        
        # é•·åº¦æª¢æŸ¥
        orig_len = len(original)
        trans_len = len(translation)
        
        if trans_len < self.rules["content_check"]["min_translation_length"]:
            issues.append("âŒ ç¿»è­¯å…§å®¹éçŸ­")
            score -= 40
            
        if orig_len > 0:
            length_ratio = trans_len / orig_len
            max_ratio = self.rules["content_check"]["max_length_ratio"]
            min_ratio = self.rules["content_check"]["min_length_ratio"]
            
            if length_ratio > max_ratio:
                issues.append(f"âš ï¸  ç¿»è­¯éé•·ï¼ˆæ¯”ä¾‹: {length_ratio:.1f}ï¼Œå»ºè­°: <{max_ratio}ï¼‰")
                score -= 15
            elif length_ratio < min_ratio:
                issues.append(f"âš ï¸  ç¿»è­¯éçŸ­ï¼ˆæ¯”ä¾‹: {length_ratio:.1f}ï¼Œå»ºè­°: >{min_ratio}ï¼‰")
                score -= 15
                
        # æª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„ç¿»è­¯æ¨¡æ¿ç—•è·¡
        template_indicators = ["[æ­¤è™•æ‡‰ç‚º", "[å¾…è£œå……]", "è«‹ä½¿ç”¨AIç¿»è­¯"]
        for indicator in template_indicators:
            if indicator in translation:
                issues.append(f"âŒ ç¿»è­¯ä¸­åŒ…å«æ¨¡æ¿æ¨™è¨˜: {indicator}")
                score -= 30
                
        return {"score": max(0, score), "issues": issues}
        
    def check_terminology(self, translation: str) -> Dict:
        """æª¢æŸ¥è¡“èªä½¿ç”¨"""
        issues = []
        suggestions = []
        score = 100
        
        # æª¢æŸ¥ç¦ç”¨è©å½™
        for term, info in self.terminology.items():
            if info.get("category") == "ç¦ç”¨" and term in translation:
                issues.append(f"âŒ ä½¿ç”¨äº†ä¸ç•¶è©å½™: ã€Œ{term}ã€")
                if "correct" in info:
                    suggestions.append(f"ğŸ’¡ å»ºè­°å°‡ã€Œ{term}ã€æ”¹ç‚ºã€Œ{info['correct']}ã€")
                score -= 20
                
        # æª¢æŸ¥è¡“èªä¸€è‡´æ€§
        used_terms = {}
        for term, info in self.terminology.items():
            if info.get("category") != "ç¦ç”¨" and term in translation:
                used_terms[term] = info
                
        # æª¢æŸ¥æ˜¯å¦æœ‰æ›´å¥½çš„è¡“èªé¸æ“‡
        for term, info in used_terms.items():
            if "alternatives" in info and len(info["alternatives"]) > 1:
                suggestions.append(f"ğŸ’¡ ã€Œ{term}ã€å¯è€ƒæ…®ä½¿ç”¨: {', '.join(info['alternatives'])}")
                
        return {"score": max(0, score), "issues": issues, "suggestions": suggestions}
        
    def generate_evaluation_report(self, results: Dict) -> str:
        """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
        score = results["overall_score"]
        
        # è©•ç´š
        if score >= 90:
            grade = "å„ªç§€ â­â­â­â­â­"
        elif score >= 80:
            grade = "è‰¯å¥½ â­â­â­â­"
        elif score >= 70:
            grade = "åŠæ ¼ â­â­â­"
        elif score >= 60:
            grade = "å¾…æ”¹é€² â­â­"
        else:
            grade = "ä¸åŠæ ¼ â­"
            
        report = f"""# ğŸ“Š AIç¿»è­¯å“è³ªè©•ä¼°å ±å‘Š

## ğŸ¯ ç¸½é«”è©•ä¼°
- **ç¸½åˆ†**: {score:.1f}/100
- **è©•ç´š**: {grade}

## ğŸ“‹ è©³ç´°è©•åˆ†
- **æ ¼å¼è¦ç¯„**: {results['format_score']:.1f}/100
- **å…§å®¹å“è³ª**: {results['content_score']:.1f}/100  
- **è¡“èªä½¿ç”¨**: {results['terminology_score']:.1f}/100

"""
        
        # å•é¡Œåˆ—è¡¨
        if results["issues"]:
            report += "## âŒ ç™¼ç¾çš„å•é¡Œ\n\n"
            for issue in results["issues"]:
                report += f"- {issue}\n"
            report += "\n"
            
        # å»ºè­°åˆ—è¡¨
        if results["suggestions"]:
            report += "## ğŸ’¡ æ”¹é€²å»ºè­°\n\n"
            for suggestion in results["suggestions"]:
                report += f"- {suggestion}\n"
            report += "\n"
            
        # è©•ä¼°æ¨™æº–
        report += """## ğŸ“ è©•ä¼°æ¨™æº–

### æ ¼å¼è¦ç¯„ (30%)
- åŒ…å«å¿…è¦éƒ¨åˆ†ï¼šåŸæ–‡ã€ç¿»è­¯ã€è¨»è§£
- è¨»è§£åŒ…å«ï¼šé‡è¦è©å½™ã€æ–‡åŒ–èƒŒæ™¯ã€ç¿»è­¯è¦é»
- éµå¾ªæ¨™æº–Markdownæ ¼å¼

### å…§å®¹å“è³ª (40%)
- ç¿»è­¯é•·åº¦é©ä¸­ï¼ˆåŸæ–‡30%-300%ï¼‰
- ç„¡æ¨¡æ¿æ¨™è¨˜æ®˜ç•™
- å…§å®¹å®Œæ•´æº–ç¢º

### è¡“èªä½¿ç”¨ (30%)
- é¿å…ä½¿ç”¨ä¸ç•¶è©å½™
- é“æ•™è¡“èªä½¿ç”¨æ­£ç¢º
- è¡“èªä½¿ç”¨ä¸€è‡´

---
*è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report


def evaluate_translation_file(file_path: str) -> None:
    """è©•ä¼°ç¿»è­¯æª”æ¡ˆ"""
    evaluator = TranslationEvaluator()
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        results = evaluator.evaluate_translation(content)
        report = evaluator.generate_evaluation_report(results)
        
        # å„²å­˜è©•ä¼°å ±å‘Š
        report_path = Path(file_path).with_suffix('.evaluation.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(f"âœ… è©•ä¼°å®Œæˆ: {file_path}")
        print(f"ğŸ“Š ç¸½åˆ†: {results['overall_score']:.1f}/100")
        print(f"ğŸ“‹ å ±å‘Š: {report_path}")
        
    except Exception as e:
        print(f"âŒ è©•ä¼°å¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AIç¿»è­¯å“è³ªè©•ä¼°å·¥å…·")
    parser.add_argument("file", help="è¦è©•ä¼°çš„ç¿»è­¯æª”æ¡ˆè·¯å¾‘")
    
    args = parser.parse_args()
    
    if not Path(args.file).exists():
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.file}")
        return 1
        
    evaluate_translation_file(args.file)
    return 0


if __name__ == "__main__":
    exit(main())