#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é“æ•™ç¶“å…¸ç¿»è­¯ç³»çµ± - æ ¸å¿ƒç¿»è­¯å™¨

æ•´åˆåŸæœ‰çš„ auto_translator.py åŠŸèƒ½ï¼Œæä¾›çµ±ä¸€çš„ç¿»è­¯ä»‹é¢
"""

import requests
import re
import json
import time
from pathlib import Path
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List, Optional, Tuple

from .tracker import ClassicTracker
from .file_monitor import FileMonitor


class TranslationEngine:
    """ç¿»è­¯å¼•æ“æ ¸å¿ƒé¡"""
    
    def __init__(self, config: Dict = None):
        """åˆå§‹åŒ–ç¿»è­¯å¼•æ“"""
        self.config = config or self._load_default_config()
        self.session = self._create_session()
        self.tracker = ClassicTracker()
        self.file_monitor = FileMonitor()
        
        # åˆå§‹åŒ–ç‹€æ…‹
        self.current_book = None
        self.project_root = None
        self.source_dir = None
        self.translation_dir = None
        
    def _load_default_config(self) -> Dict:
        """è¼‰å…¥é è¨­é…ç½®"""
        return {
            "base_url": "https://www.shidianguji.com",
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "request_delay": 2,
            "max_retries": 3,
            "timeout": 10
        }
        
    def _create_session(self) -> requests.Session:
        """å‰µå»ºHTTPæœƒè©±"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': self.config["user_agent"]
        })
        return session
        
    def extract_book_id(self, url: str) -> Optional[str]:
        """å¾URLæå–æ›¸ç±ID"""
        match = re.search(r'/book/([^/?]+)', url)
        return match.group(1) if match else None
        
    def get_book_info(self, book_url: str) -> Dict:
        """ç²å–æ›¸ç±åŸºæœ¬è³‡è¨Š"""
        try:
            response = self.session.get(book_url, timeout=self.config["timeout"])
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            book_id = self.extract_book_id(book_url)
            
            # æå–æ›¸ç±æ¨™é¡Œ
            book_title = self._extract_title(soup, book_id)
            
            # æå–ä½œè€…è³‡è¨Š
            author = self._extract_author(soup)
            
            return {
                'id': book_id,
                'title': book_title,
                'author': author,
                'url': book_url
            }
            
        except Exception as e:
            print(f"âš ï¸  ç²å–æ›¸ç±è³‡è¨Šå¤±æ•—: {e}")
            return {
                'id': self.extract_book_id(book_url),
                'title': self.extract_book_id(book_url),
                'author': "æœªçŸ¥ä½œè€…",
                'url': book_url
            }     
       
    def _extract_title(self, soup: BeautifulSoup, book_id: str) -> str:
        """æå–æ›¸ç±æ¨™é¡Œ"""
        title_selectors = [
            'h1.Goq6DYSE',
            'h1',
            '.book-title h1',
            '.title',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text().strip()
                # æ¸…ç†æ¨™é¡Œ
                title_text = re.sub(r'[-â€“â€”]\s*è­˜å…¸å¤ç±.*', '', title_text)
                title_text = re.sub(r'\s*\|\s*.*', '', title_text)
                if len(title_text) > 2 and title_text != book_id:
                    return title_text
                    
        return book_id
        
    def _extract_author(self, soup: BeautifulSoup) -> str:
        """æå–ä½œè€…è³‡è¨Š"""
        author_selectors = [
            '.book-title-author',
            '.author',
            '[class*="author"]'
        ]
        
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                author_text = author_elem.get_text().strip()
                if author_text and len(author_text) > 1:
                    return author_text
                    
        # å˜—è©¦å¾æ–‡æœ¬ä¸­æœå°‹
        text_content = soup.get_text()
        author_patterns = [
            r'[\[ã€\(ï¼ˆ]([^ã€‘\]ï¼‰\)]*(?:æ’°|è‘—|ç·¨|è¼¯))[\]ã€‘\)ï¼‰]',
            r'([^ï¼Œã€‚ï¼›ï¼š\s]{2,6})\s*(?:æ’°|è‘—|ç·¨|è¼¯)',
        ]
        for pattern in author_patterns:
            match = re.search(pattern, text_content)
            if match:
                return match.group(1)
                
        return "æœªçŸ¥ä½œè€…"
        
    def setup_project_structure(self, book_info: Dict) -> None:
        """è¨­å®šå°ˆæ¡ˆçµæ§‹"""
        # æ¸…ç†æ›¸åï¼Œç§»é™¤ä¸é©åˆä½œç‚ºè³‡æ–™å¤¾åç¨±çš„å­—ç¬¦
        clean_title = re.sub(r'[<>:"/\\|?*]', '_', book_info['title'])
        clean_title = re.sub(r'\s+', '_', clean_title)
        folder_name = f"{clean_title}_{book_info['id']}"
        
        self.project_root = Path(f"docs/source_texts/{folder_name}")
        self.source_dir = self.project_root / "åŸæ–‡"
        self.translation_dir = Path(f"docs/translations/{folder_name}")
        
        # å»ºç«‹ç›®éŒ„
        self.source_dir.mkdir(parents=True, exist_ok=True)
        self.translation_dir.mkdir(parents=True, exist_ok=True)
        
        self.current_book = book_info
        
    def get_chapter_list(self, book_url: str) -> List[Dict]:
        """ç²å–ç« ç¯€åˆ—è¡¨"""
        print(f"ğŸ” æ­£åœ¨ç²å–ç« ç¯€åˆ—è¡¨...")
        
        try:
            response = self.session.get(book_url, timeout=self.config["timeout"])
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            chapters = []
            chapter_links = soup.find_all('a', href=re.compile(r'/chapter/'))
            
            for idx, link in enumerate(chapter_links, 1):
                href = link.get('href')
                title = link.get_text().strip()
                
                if href and title and len(title) > 2:
                    full_url = self.config["base_url"] + href if href.startswith('/') else href
                    chapters.append({
                        'number': idx,
                        'title': title,
                        'url': full_url,
                        'chapter_id': self._extract_chapter_id(href)
                    })
                    
            if chapters:
                print(f"âœ… æˆåŠŸç²å– {len(chapters)} å€‹ç« ç¯€")
                return chapters
            else:
                print("âš ï¸  æœªæ‰¾åˆ°ç« ç¯€ï¼Œå˜—è©¦APIæ–¹å¼...")
                return self._get_chapters_via_api()
                
        except Exception as e:
            print(f"âŒ ç²å–ç« ç¯€åˆ—è¡¨å¤±æ•—: {e}")
            return []
            
    def _extract_chapter_id(self, href: str) -> Optional[str]:
        """å¾hrefæå–ç« ç¯€ID"""
        match = re.search(r'/chapter/([^/?]+)', href)
        return match.group(1) if match else None
        
    def _get_chapters_via_api(self) -> List[Dict]:
        """é€šéAPIæ–¹å¼ç²å–ç« ç¯€åˆ—è¡¨"""
        # é ç•™APIå¯¦ç¾
        return []
        
    def crawl_chapter(self, chapter_info: Dict) -> Optional[Dict]:
        """çˆ¬å–å–®ä¸€ç« ç¯€"""
        print(f"ğŸ“– çˆ¬å–ç« ç¯€: {chapter_info['title']}")
        
        try:
            api_url = f"{self.config['base_url']}/api/book/{self.current_book['id']}/chapter/{chapter_info['chapter_id']}"
            
            response = self.session.get(api_url, timeout=self.config["timeout"])
            if response.status_code != 200:
                print(f"âŒ APIè«‹æ±‚å¤±æ•—: {response.status_code}")
                return None
                
            soup = BeautifulSoup(response.text, 'html.parser')
            content_data = self._extract_content_from_html(soup, chapter_info['title'])
            
            if content_data:
                self._save_source_text(content_data, chapter_info['number'])
                return content_data
            else:
                print(f"âŒ ç„¡æ³•æå–ç« ç¯€å…§å®¹: {chapter_info['title']}")
                return None
                
        except Exception as e:
            print(f"âŒ çˆ¬å–ç« ç¯€å¤±æ•—: {e}")
            return None
            
    def _extract_content_from_html(self, soup: BeautifulSoup, title: str) -> Optional[Dict]:
        """å¾HTMLä¸­æå–å…§å®¹"""
        try:
            main_content = soup.find('main', class_='read-layout-main')
            if main_content:
                article = main_content.find('article', class_='chapter-reader')
                if article:
                    content_parts = []
                    
                    for element in article.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):
                        text = element.get_text().strip()
                        if text and len(text) > 3:
                            content_parts.append(text)
                    
                    if content_parts:
                        return {
                            'title': title,
                            'content': '\n\n'.join(content_parts)
                        }
                        
        except Exception as e:
            print(f"âš ï¸  HTMLè§£æéŒ¯èª¤: {e}")
            
        return None
        
    def _save_source_text(self, content_data: Dict, chapter_number: int) -> None:
        """å„²å­˜åŸæ–‡"""
        filename = f"{chapter_number:02d}_{content_data['title']}.txt"
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        file_path = self.source_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {content_data['title']}\n\n")
            f.write(content_data['content'])
            
        # è¨˜éŒ„æª”æ¡ˆæ“ä½œ
        self.file_monitor.track_file_write(file_path, "source_text", {
            "chapter_number": chapter_number,
            "title": content_data['title'],
            "content_length": len(content_data['content'])
        })
            
        print(f"âœ… å·²å„²å­˜åŸæ–‡: {filename}")
        
    def generate_translation_template(self, content_data: Dict, chapter_number: int) -> None:
        """ç”Ÿæˆç¿»è­¯æ¨¡æ¿"""
        print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆç¿»è­¯æ¨¡æ¿: {content_data['title']}")
        
        filename = f"{chapter_number:02d}_{content_data['title']}.md"
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        file_path = self.translation_dir / filename
        
        markdown_content = f"""# {content_data['title']}

## åŸæ–‡

{content_data['content']}

## ç¿»è­¯

[æ­¤è™•æ‡‰ç‚ºç¾ä»£ä¸­æ–‡ç¿»è­¯]

åŸæ–‡å­—æ•¸ï¼š{len(content_data['content'])} å­—
å»ºè­°ï¼šè«‹ä½¿ç”¨AIç¿»è­¯å·¥å…·æˆ–äººå·¥ç¿»è­¯æ­¤æ®µè½ã€‚

ç¿»è­¯è¦é»ï¼š
1. ä¿æŒåŸæ–‡æ„æ€
2. ä½¿ç”¨ç¾ä»£ä¸­æ–‡è¡¨é”
3. ä¿ç•™é‡è¦çš„å¤ä»£è¡“èª
4. æ·»åŠ å¿…è¦çš„è¨»è§£èªªæ˜

## è¨»è§£

**é‡è¦è©å½™ï¼š**
- [å¾…è£œå……]

**æ–‡åŒ–èƒŒæ™¯ï¼š**
- [å¾…è£œå……]

**ç¿»è­¯è¦é»ï¼š**
- [å¾…è£œå……]

---
*ç¿»è­¯æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç¿»è­¯æ–¹å¼ï¼šè‡ªå‹•ç”Ÿæˆæ¨¡æ¿*
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
            
        # è¨˜éŒ„æª”æ¡ˆæ“ä½œ
        self.file_monitor.track_file_write(file_path, "translation_template", {
            "chapter_number": chapter_number,
            "title": content_data['title'],
            "template_type": "auto_generated"
        })
            
        print(f"âœ… å·²ç”Ÿæˆç¿»è­¯æ¨¡æ¿: {filename}")
        
    def create_project_readme(self, chapters: List[Dict]) -> None:
        """å»ºç«‹å°ˆæ¡ˆèªªæ˜æª”æ¡ˆ"""
        book_info = self.current_book
        
        readme_content = f"""# {book_info['title']}

## æ›¸ç±è³‡è¨Š

- **æ›¸å**ï¼š{book_info['title']}
- **ä½œè€…**ï¼š{book_info['author']}
- **æ›¸ç±ID**ï¼š{book_info['id']}
- **åŸå§‹ç¶²å€**ï¼š{book_info['url']}

## å°ˆæ¡ˆèªªæ˜

æœ¬å°ˆæ¡ˆä½¿ç”¨é“æ•™ç¶“å…¸ç¿»è­¯ç³»çµ±ç”Ÿæˆï¼ŒåŒ…å«ï¼š
1. è‡ªå‹•çˆ¬å–çš„å¤æ–‡åŸæ–‡
2. è‡ªå‹•ç”Ÿæˆçš„ç¿»è­¯æ¨¡æ¿
3. å®Œæ•´çš„å°ˆæ¡ˆçµæ§‹

## ç« ç¯€åˆ—è¡¨

ç¸½å…± {len(chapters)} å€‹ç« ç¯€ï¼š

"""
        
        for chapter in chapters:
            readme_content += f"- {chapter['number']:02d}. {chapter['title']}\n"
            
        readme_content += f"""

## ä½¿ç”¨èªªæ˜

1. **åŸæ–‡æª”æ¡ˆ**ï¼šä½æ–¼ `åŸæ–‡/` ç›®éŒ„
2. **ç¿»è­¯æª”æ¡ˆ**ï¼šä½æ–¼ `../translations/{book_info['id']}/` ç›®éŒ„
3. **ç¿»è­¯æ¨¡æ¿**ï¼šå·²è‡ªå‹•ç”Ÿæˆï¼Œå¯ç›´æ¥ç·¨è¼¯

## ç¿»è­¯é€²åº¦

- [x] åŸæ–‡çˆ¬å–å®Œæˆ
- [x] ç¿»è­¯æ¨¡æ¿ç”Ÿæˆå®Œæˆ
- [ ] äººå·¥ç¿»è­¯å¾…å®Œæˆ

---
*å°ˆæ¡ˆå»ºç«‹æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ä½¿ç”¨å·¥å…·ï¼šé“æ•™ç¶“å…¸ç¿»è­¯ç³»çµ± v2.0*
"""
        
        readme_path = self.project_root / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
            
        # è¨˜éŒ„æª”æ¡ˆæ“ä½œ
        self.file_monitor.track_file_write(readme_path, "project_readme", {
            "book_id": book_info['id'],
            "book_title": book_info['title'],
            "chapter_count": len(chapters)
        })
            
        print(f"âœ… å·²å»ºç«‹å°ˆæ¡ˆèªªæ˜: {readme_path}")
        
    def translate_book(self, book_url: str) -> bool:
        """ç¿»è­¯æ•´æœ¬æ›¸ç±çš„ä¸»è¦æµç¨‹"""
        print("ğŸš€ å•Ÿå‹•é“æ•™ç¶“å…¸ç¿»è­¯ç³»çµ± v2.0")
        print("=" * 50)
        
        try:
            # 1. ç²å–æ›¸ç±è³‡è¨Š
            book_info = self.get_book_info(book_url)
            print(f"ğŸ“š æ›¸ç±ï¼š{book_info['title']}")
            print(f"ğŸ‘¤ ä½œè€…ï¼š{book_info['author']}")
            
            # 2. è¨­å®šå°ˆæ¡ˆçµæ§‹
            self.setup_project_structure(book_info)
            
            # 3. ç²å–ç« ç¯€åˆ—è¡¨
            chapters = self.get_chapter_list(book_url)
            if not chapters:
                print("âŒ ç„¡æ³•ç²å–ç« ç¯€åˆ—è¡¨ï¼Œç¨‹åºçµ‚æ­¢")
                return False
                
            print(f"ğŸ“‹ æ‰¾åˆ° {len(chapters)} å€‹ç« ç¯€")
            
            # 4. æ‰¹é‡çˆ¬å–å’Œç¿»è­¯
            success_count = 0
            for chapter in chapters:
                print(f"\nğŸ”„ è™•ç†ç¬¬ {chapter['number']} ç« ...")
                
                # çˆ¬å–åŸæ–‡
                content_data = self.crawl_chapter(chapter)
                if content_data:
                    # ç”Ÿæˆç¿»è­¯æ¨¡æ¿
                    self.generate_translation_template(content_data, chapter['number'])
                    success_count += 1
                    
                # æ·»åŠ å»¶é²é¿å…è¢«å°é–
                time.sleep(self.config["request_delay"])
                
            # 5. å»ºç«‹å°ˆæ¡ˆæ–‡æª”
            self.create_project_readme(chapters)
            
            # 6. è¿½è¹¤æ–°ç¶“å…¸åˆ°ç³»çµ±
            if success_count > 0:
                print("\nğŸ“Š æ›´æ–°ç¶“å…¸è¿½è¹¤ç³»çµ±...")
                try:
                    processed_chapters = []
                    for i, chapter in enumerate(chapters[:success_count], 1):
                        processed_chapters.append({
                            'number': i,
                            'title': chapter.get('title', f'ç¬¬{i}ç« '),
                            'url': chapter.get('url', '')
                        })
                    
                    self.tracker.track_new_classic(
                        book_info=book_info,
                        chapters=processed_chapters,
                        source_dir=self.project_root,
                        translation_dir=self.translation_dir
                    )
                    
                    print("âœ… ç¶“å…¸è¿½è¹¤ç³»çµ±å·²æ›´æ–°")
                    
                except Exception as e:
                    print(f"âš ï¸  è¿½è¹¤ç³»çµ±æ›´æ–°å¤±æ•—: {e}")
            
            # 7. ç”Ÿæˆç¸½çµå ±å‘Š
            print(f"\nğŸ‰ ç¿»è­¯å®Œæˆï¼")
            print(f"âœ… æˆåŠŸè™•ç†ï¼š{success_count}/{len(chapters)} ç« ")
            print(f"ğŸ“ å°ˆæ¡ˆä½ç½®ï¼š{self.project_root}")
            print(f"ğŸ“ ç¿»è­¯æª”æ¡ˆï¼š{self.translation_dir}")
            
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ ç¿»è­¯éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False