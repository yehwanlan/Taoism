# DZ1439 洞玄靈寶玉京山步虛經 爬蟲使用說明

## 📚 爬取成果

成功爬取 **師典古籍網站** 的 DZ1439 經文：

- **書名**: 洞玄灵宝玉京山步虚经
- **朝代**: 东晋
- **作者**: 佚名
- **章節數**: 7 章
- **總字數**: 3,502 字
- **成功率**: 100% (7/7 章)

## 🎯 爬蟲特點

### 核心技術
- **靜態爬蟲**: 使用 `requests` + `BeautifulSoup`
- **內容提取**: 使用 `<article>` 和 `<main>` 標籤定位內容
- **編碼處理**: UTF-8 編碼，正確處理古文漢字
- **禮貌爬取**: 每次請求間隔 2 秒，避免伺服器負擔

### 關鍵發現
1. **頁面結構**: 師典古籍使用 React 前端框架，但內容已在 HTML 中渲染
2. **內容定位**: 經文內容包含在 `<article>` 標籤內
3. **章節導航**: 使用 `semi-tree-option` class 的 div 元素
4. **登入提示**: 頁面顯示「登录后阅读更方便」，但基本內容無需登入即可訪問

## 📁 輸出檔案

### 1. JSON 格式 (`dz1439_complete.json`)
完整的結構化資料，包含：
- 書籍基本資訊（書名、作者、朝代、摘要）
- 所有章節的標題、URL 和內容

### 2. 文字檔案 (`dz1439_output/`)
```
dz1439_output/
├── 00_書籍資訊.txt          # 書籍基本資訊
├── 01_洞玄灵宝玉京山步虚经.txt
├── 02_步虚吟.txt
├── 03_洞玄步虚吟十首.txt
├── 04_太上智慧经赞八首.txt
├── 05_玄师太元真人临授许常侍、掾《太洞玄经玉京山诀》，作颂三首.txt
├── 06_太上太极五真人于会稽山、虞山授葛仙公《洞玄灵宝经》，各吟一颂.txt
└── 07_礼经三首咒.txt
```

## 🚀 使用方法

### 快速開始
```bash
# 執行爬蟲
python crawler_dz1439_final.py
```

### 程式碼結構
```python
class DZ1439Crawler:
    def get_book_info()           # 獲取書籍資訊和章節列表
    def get_chapter_content()     # 獲取單個章節內容
    def crawl_all_chapters()      # 爬取所有章節
    def save_to_json()            # 保存為 JSON
    def save_to_text_files()      # 保存為文字檔案
```

### 自訂參數
```python
# 修改延遲時間（預設 2 秒）
book_info = crawler.crawl_all_chapters(book_info, delay=3)

# 修改輸出檔名
crawler.save_to_json(book_info, 'my_output.json')
crawler.save_to_text_files(book_info, 'my_output_dir')
```

## 🔧 技術細節

### 請求標頭
```python
headers = {
    'User-Agent': 'Mozilla/5.0 ...',
    'Accept': 'text/html,application/xhtml+xml...',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Referer': 'https://www.shidianguji.com/'
}
```

### 內容提取邏輯
```python
# 優先使用 article 標籤
article_tag = soup.find('article')
if article_tag:
    content = article_tag.get_text(separator='\n', strip=True)

# 備用方案：使用 main 標籤
if not content:
    main_tag = soup.find('main')
    if main_tag:
        content = main_tag.get_text(separator='\n', strip=True)
```

### 章節列表提取
```python
# 查找所有章節連結
chapter_items = soup.find_all('div', class_='semi-tree-option')
for item in chapter_items:
    a_tag = item.find('a')
    if a_tag:
        chapter_name = a_tag.text.strip()
        chapter_url = a_tag.get('href', '')
```

## 📊 爬取統計

| 項目 | 數值 |
|------|------|
| 書籍編號 | DZ1439 |
| 總章節數 | 7 章 |
| 成功爬取 | 7 章 (100%) |
| 總字數 | 3,502 字 |
| 平均每章 | 500 字 |
| 爬取時間 | 約 15 秒 |

## ⚠️ 注意事項

1. **遵守 robots.txt**: 師典古籍網站允許爬取
2. **禮貌爬取**: 設定適當的延遲時間（建議 2-3 秒）
3. **僅供學習**: 爬取的內容僅用於個人學習和研究
4. **版權尊重**: 不得用於商業用途
5. **內容限制**: 部分內容可能需要登入才能完整訪問

## 🔄 擴展應用

### 1. 爬取其他書籍
修改 `book_id` 即可：
```python
crawler = DZ1439Crawler()
crawler.book_id = "DZ1234"  # 改為其他書籍編號
```

### 2. 整合到現有系統
```python
# 整合到 Taoism 專案
from crawler_dz1439_final import DZ1439Crawler

crawler = DZ1439Crawler()
book_info = crawler.get_book_info()
book_info = crawler.crawl_all_chapters(book_info)

# 保存到專案的 docs/source_texts/ 目錄
crawler.save_to_text_files(book_info, 'docs/source_texts/DZ1439')
```

### 3. 批量爬取
```python
book_ids = ['DZ1439', 'DZ1234', 'DZ1437']
for book_id in book_ids:
    crawler = DZ1439Crawler()
    crawler.book_id = book_id
    # ... 執行爬取
```

## 📝 測試檔案

專案包含多個測試檔案：

1. `test_dz1439_crawler.py` - 基本爬取測試
2. `test_dz1439_chapter_detail.py` - 頁面結構分析
3. `crawler_dz1439_final.py` - 完整爬蟲（推薦使用）

## 🎓 學習要點

### Python 爬蟲技術
- HTTP 請求處理 (`requests`)
- HTML 解析 (`BeautifulSoup`)
- 資料結構設計（JSON、文字檔案）
- 錯誤處理和重試機制
- 編碼處理（UTF-8、古文漢字）

### 網頁分析技巧
- 使用瀏覽器開發者工具檢查 HTML 結構
- 識別內容容器標籤（article, main, div）
- 處理動態網頁（React、Vue）
- 分析 CSS 選擇器和 class 名稱

### 最佳實踐
- 設定合理的請求延遲
- 使用 User-Agent 模擬瀏覽器
- 實作錯誤處理和日誌記錄
- 結構化保存資料（JSON + 文字檔案）

## 🔗 相關資源

- 師典古籍網站: https://www.shidianguji.com
- DZ1439 書籍頁面: https://www.shidianguji.com/book/DZ1439
- Python Requests 文檔: https://requests.readthedocs.io/
- BeautifulSoup 文檔: https://www.crummy.com/software/BeautifulSoup/

## ✅ 完成！

爬蟲已成功完成，所有資料已保存。你可以：
- 查看 `dz1439_complete.json` 了解完整資料結構
- 閱讀 `dz1439_output/` 目錄中的文字檔案
- 修改程式碼以爬取其他書籍
- 整合到你的道教經典翻譯系統中
