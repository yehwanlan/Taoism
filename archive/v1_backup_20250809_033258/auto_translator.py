#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨è‡ªå‹•å¤ç±ç¿»è­¯ç³»çµ±

åŠŸèƒ½ï¼š
1. è‡ªå‹•è®€å–æ›¸ç±ç›®éŒ„
2. æ‰¹é‡çˆ¬å–æ‰€æœ‰ç« ç¯€
3. è‡ªå‹•AIç¿»è­¯
4. ç”Ÿæˆå®Œæ•´çš„ç¿»è­¯å°ˆæ¡ˆ
"""

import requests
import re
import json
import time
from pathlib import Path
from bs4 import BeautifulSoup
from datetime import datetime
from classic_tracker import track_new_classic, generate_tracking_report
from file_tracker import track_file_write, log_operation

class AutoTranslator:
    """å…¨è‡ªå‹•å¤ç±ç¿»è­¯ç³»çµ±"""
    
    def __init__(self, book_url):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # å¾URLæå–æ›¸ç±è³‡è¨Š
        self.book_url = book_url
        self.book_id = self.extract_book_id(book_url)
        self.base_url = "https://www.shidianguji.com"
        
        # åˆå§‹åŒ–æ™‚ä¸è¨­å®šå°ˆæ¡ˆçµæ§‹ï¼Œç­‰ç²å–æ›¸ç±è³‡è¨Šå¾Œå†è¨­å®š
        self.project_root = None
        self.source_dir = None
        self.translation_dir = None
        self.folder_name = None
        
    def extract_book_id(self, url):
        """å¾URLæå–æ›¸ç±ID"""
        match = re.search(r'/book/([^/?]+)', url)
        return match.group(1) if match else None
        
    def setup_project_structure(self, book_info=None):
        """è¨­å®šå°ˆæ¡ˆçµæ§‹"""
        if not self.book_id:
            raise ValueError("ç„¡æ³•å¾URLæå–æ›¸ç±ID")
        
        # å¦‚æœæä¾›äº†æ›¸ç±è³‡è¨Šï¼Œä½¿ç”¨æ›¸åä½œç‚ºè³‡æ–™å¤¾åç¨±
        if book_info and book_info['title'] != self.book_id:
            # æ¸…ç†æ›¸åï¼Œç§»é™¤ä¸é©åˆä½œç‚ºè³‡æ–™å¤¾åç¨±çš„å­—ç¬¦
            clean_title = re.sub(r'[<>:"/\\|?*]', '_', book_info['title'])
            clean_title = re.sub(r'\s+', '_', clean_title)  # ç©ºæ ¼æ›¿æ›ç‚ºä¸‹åŠƒç·š
            folder_name = f"{clean_title}_{self.book_id}"  # æ›¸å_IDæ ¼å¼
        else:
            folder_name = self.book_id
            
        self.project_root = Path(f"docs/source_texts/{folder_name}")
        self.source_dir = self.project_root / "åŸæ–‡"
        self.translation_dir = Path(f"docs/translations/{folder_name}")
        
        # å»ºç«‹ç›®éŒ„
        self.source_dir.mkdir(parents=True, exist_ok=True)
        self.translation_dir.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜è³‡æ–™å¤¾åç¨±ä¾›å¾ŒçºŒä½¿ç”¨
        self.folder_name = folder_name
        
    def get_book_info(self):
        """ç²å–æ›¸ç±åŸºæœ¬è³‡è¨Š"""
        try:
            response = self.session.get(self.book_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ›¸ç±æ¨™é¡Œ - æ”¹é€²ç‰ˆæœ¬
            book_title = self.book_id  # é è¨­å€¼
            
            # å˜—è©¦å¤šç¨®æ–¹å¼æå–æ¨™é¡Œ
            title_selectors = [
                'h1.Goq6DYSE',  # åå…¸å¤ç±ç¶²çš„ç‰¹å®šé¡åˆ¥
                'h1',
                '.book-title h1',
                '.title',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    # æ¸…ç†æ¨™é¡Œï¼Œç§»é™¤ç¶²ç«™åç¨±ç­‰
                    title_text = re.sub(r'[-â€“â€”]\s*è­˜å…¸å¤ç±.*$', '', title_text)
                    title_text = re.sub(r'\s*\|\s*.*$', '', title_text)
                    if len(title_text) > 2 and title_text != self.book_id:
                        book_title = title_text
                        break
            
            # æå–ä½œè€…è³‡è¨Š - æ”¹é€²ç‰ˆæœ¬
            author = "æœªçŸ¥ä½œè€…"
            author_selectors = [
                '.book-title-author',
                '.author',
                '[class*="author"]'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    author_text = author_elem.get_text().strip()
                    if author_text and len(author_text) > 1:
                        author = author_text
                        break
            
            # å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ä½œè€…ï¼Œå˜—è©¦å¾æ–‡æœ¬ä¸­æœå°‹
            if author == "æœªçŸ¥ä½œè€…":
                text_content = soup.get_text()
                author_patterns = [
                    r'[\[ã€\(ï¼ˆ]([^ã€‘\]ï¼‰\)]*(?:æ’°|è‘—|ç·¨|è¼¯))[\]ã€‘\)ï¼‰]',
                    r'([^ï¼Œã€‚ï¼›ï¼š\s]{2,6})\s*(?:æ’°|è‘—|ç·¨|è¼¯)',
                ]
                for pattern in author_patterns:
                    match = re.search(pattern, text_content)
                    if match:
                        author = match.group(1)
                        break
            
            return {
                'id': self.book_id,
                'title': book_title,
                'author': author,
                'url': self.book_url
            }
            
        except Exception as e:
            print(f"âš ï¸  ç²å–æ›¸ç±è³‡è¨Šå¤±æ•—: {e}")
            return {
                'id': self.book_id,
                'title': self.book_id,
                'author': "æœªçŸ¥ä½œè€…",
                'url': self.book_url
            }
            
    def get_chapter_list(self):
        """è‡ªå‹•ç²å–å®Œæ•´ç« ç¯€åˆ—è¡¨"""
        print(f"ğŸ” æ­£åœ¨ç²å– {self.book_id} çš„ç« ç¯€åˆ—è¡¨...")
        
        try:
            # æ–¹æ³•1: å˜—è©¦å¾æ›¸ç±ä¸»é ç²å–
            response = self.session.get(self.book_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            chapters = []
            
            # å°‹æ‰¾ç« ç¯€é€£çµ
            chapter_links = soup.find_all('a', href=re.compile(r'/chapter/'))
            
            for idx, link in enumerate(chapter_links, 1):
                href = link.get('href')
                title = link.get_text().strip()
                
                if href and title and len(title) > 2:
                    full_url = self.base_url + href if href.startswith('/') else href
                    chapters.append({
                        'number': idx,
                        'title': title,
                        'url': full_url,
                        'chapter_id': self.extract_chapter_id(href)
                    })
                    
            if chapters:
                print(f"âœ… æˆåŠŸç²å– {len(chapters)} å€‹ç« ç¯€")
                return chapters
                
            # æ–¹æ³•2: å¦‚æœä¸»é æ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦APIæ–¹å¼
            print("ğŸ”„ å˜—è©¦é€šéAPIç²å–ç« ç¯€åˆ—è¡¨...")
            return self.get_chapters_via_api()
            
        except Exception as e:
            print(f"âŒ ç²å–ç« ç¯€åˆ—è¡¨å¤±æ•—: {e}")
            return []
            
    def extract_chapter_id(self, href):
        """å¾hrefæå–ç« ç¯€ID"""
        match = re.search(r'/chapter/([^/?]+)', href)
        return match.group(1) if match else None
        
    def get_chapters_via_api(self):
        """é€šéAPIæ–¹å¼ç²å–ç« ç¯€åˆ—è¡¨"""
        # é€™è£¡å¯ä»¥å¯¦ä½œæ›´è¤‡é›œçš„APIèª¿ç”¨é‚è¼¯
        # æš«æ™‚è¿”å›ç©ºåˆ—è¡¨ï¼Œå¯¦éš›ä½¿ç”¨æ™‚å¯ä»¥æ ¹æ“šå…·é«”ç¶²ç«™APIèª¿æ•´
        return []
        
    def crawl_chapter(self, chapter_info):
        """çˆ¬å–å–®ä¸€ç« ç¯€"""
        print(f"ğŸ“– çˆ¬å–ç« ç¯€: {chapter_info['title']}")
        
        try:
            # æ§‹å»ºAPI URL
            api_url = f"{self.base_url}/api/book/{self.book_id}/chapter/{chapter_info['chapter_id']}"
            
            response = self.session.get(api_url, timeout=10)
            if response.status_code != 200:
                print(f"âŒ APIè«‹æ±‚å¤±æ•—: {response.status_code}")
                return None
                
            # è§£æHTMLå…§å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
            content_data = self.extract_content_from_html(soup, chapter_info['title'])
            
            if content_data:
                # å„²å­˜åŸæ–‡
                self.save_source_text(content_data, chapter_info['number'])
                return content_data
            else:
                print(f"âŒ ç„¡æ³•æå–ç« ç¯€å…§å®¹: {chapter_info['title']}")
                return None
                
        except Exception as e:
            print(f"âŒ çˆ¬å–ç« ç¯€å¤±æ•—: {e}")
            return None
            
    def extract_content_from_html(self, soup, title):
        """å¾HTMLä¸­æå–å…§å®¹"""
        try:
            # å°‹æ‰¾ä¸»è¦å…§å®¹å€åŸŸ
            main_content = soup.find('main', class_='read-layout-main')
            if main_content:
                article = main_content.find('article', class_='chapter-reader')
                if article:
                    content_parts = []
                    
                    for element in article.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):
                        text = element.get_text().strip()
                        if text and len(text) > 3:
                            content_parts.append(text)
                    
                    if content_parts:
                        return {
                            'title': title,
                            'content': '\n\n'.join(content_parts)
                        }
                        
        except Exception as e:
            print(f"âš ï¸  HTMLè§£æéŒ¯èª¤: {e}")
            
        return None
        
    def save_source_text(self, content_data, chapter_number):
        """å„²å­˜åŸæ–‡"""
        filename = f"{chapter_number:02d}_{content_data['title']}.txt"
        # æ¸…ç†æª”æ¡ˆåç¨±
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        file_path = self.source_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {content_data['title']}\n\n")
            f.write(content_data['content'])
            
        # è¿½è¹¤æª”æ¡ˆå¯«å…¥
        track_file_write(file_path, "source_text")
        log_operation("create", file_path, {
            "chapter_number": chapter_number,
            "title": content_data['title'],
            "content_length": len(content_data['content'])
        })
            
        print(f"âœ… å·²å„²å­˜åŸæ–‡: {filename}")
        
    def translate_chapter(self, content_data, chapter_number):
        """ç¿»è­¯ç« ç¯€ï¼ˆä½¿ç”¨AIæˆ–è¦å‰‡ï¼‰"""
        print(f"ğŸ¤– æ­£åœ¨ç¿»è­¯: {content_data['title']}")
        
        # é€™è£¡å¯ä»¥æ•´åˆå„ç¨®ç¿»è­¯æ–¹æ³•
        translation = self.simple_translate(content_data['content'])
        
        # ç”Ÿæˆç¿»è­¯æª”æ¡ˆ
        self.save_translation(content_data, translation, chapter_number)
        
    def simple_translate(self, ancient_text):
        """ç°¡å–®çš„ç¿»è­¯é‚è¼¯ï¼ˆå¯ä»¥æ›¿æ›ç‚ºAIç¿»è­¯ï¼‰"""
        # é€™æ˜¯ä¸€å€‹ç¤ºä¾‹ï¼Œå¯¦éš›ä½¿ç”¨æ™‚å¯ä»¥æ•´åˆï¼š
        # 1. OpenAI API
        # 2. æœ¬åœ°AIæ¨¡å‹
        # 3. å…¶ä»–ç¿»è­¯æœå‹™
        
        # æš«æ™‚è¿”å›æç¤ºæ–‡æœ¬
        return f"""[æ­¤è™•æ‡‰ç‚ºç¾ä»£ä¸­æ–‡ç¿»è­¯]

åŸæ–‡å­—æ•¸ï¼š{len(ancient_text)} å­—
å»ºè­°ï¼šè«‹ä½¿ç”¨AIç¿»è­¯å·¥å…·æˆ–äººå·¥ç¿»è­¯æ­¤æ®µè½ã€‚

ç¿»è­¯è¦é»ï¼š
1. ä¿æŒåŸæ–‡æ„æ€
2. ä½¿ç”¨ç¾ä»£ä¸­æ–‡è¡¨é”
3. ä¿ç•™é‡è¦çš„å¤ä»£è¡“èª
4. æ·»åŠ å¿…è¦çš„è¨»è§£èªªæ˜
"""
        
    def save_translation(self, content_data, translation, chapter_number):
        """å„²å­˜ç¿»è­¯"""
        filename = f"{chapter_number:02d}_{content_data['title']}.md"
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        file_path = self.translation_dir / filename
        
        markdown_content = f"""# {content_data['title']}

## åŸæ–‡

{content_data['content']}

## ç¿»è­¯

{translation}

## è¨»è§£

**é‡è¦è©å½™ï¼š**
- [å¾…è£œå……]

**æ–‡åŒ–èƒŒæ™¯ï¼š**
- [å¾…è£œå……]

**ç¿»è­¯è¦é»ï¼š**
- [å¾…è£œå……]

---
*ç¿»è­¯æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç¿»è­¯æ–¹å¼ï¼šè‡ªå‹•ç”Ÿæˆæ¨¡æ¿*
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
            
        # è¿½è¹¤ç¿»è­¯æª”æ¡ˆå¯«å…¥
        track_file_write(file_path, "translation_template")
        log_operation("create", file_path, {
            "chapter_number": chapter_number,
            "title": content_data['title'],
            "template_type": "auto_generated"
        })
            
        print(f"âœ… å·²ç”Ÿæˆç¿»è­¯æ¨¡æ¿: {filename}")
        
    def create_project_readme(self, book_info, chapters):
        """å»ºç«‹å°ˆæ¡ˆèªªæ˜æª”æ¡ˆ"""
        readme_content = f"""# {book_info['title']}

## æ›¸ç±è³‡è¨Š

- **æ›¸å**ï¼š{book_info['title']}
- **ä½œè€…**ï¼š{book_info['author']}
- **æ›¸ç±ID**ï¼š{book_info['id']}
- **åŸå§‹ç¶²å€**ï¼š{book_info['url']}

## å°ˆæ¡ˆèªªæ˜

æœ¬å°ˆæ¡ˆä½¿ç”¨å…¨è‡ªå‹•ç¿»è­¯ç³»çµ±ç”Ÿæˆï¼ŒåŒ…å«ï¼š
1. è‡ªå‹•çˆ¬å–çš„å¤æ–‡åŸæ–‡
2. è‡ªå‹•ç”Ÿæˆçš„ç¿»è­¯æ¨¡æ¿
3. å®Œæ•´çš„å°ˆæ¡ˆçµæ§‹

## ç« ç¯€åˆ—è¡¨

ç¸½å…± {len(chapters)} å€‹ç« ç¯€ï¼š

"""
        
        for chapter in chapters:
            readme_content += f"- {chapter['number']:02d}. {chapter['title']}\n"
            
        readme_content += f"""
## ä½¿ç”¨èªªæ˜

1. **åŸæ–‡æª”æ¡ˆ**ï¼šä½æ–¼ `åŸæ–‡/` ç›®éŒ„
2. **ç¿»è­¯æª”æ¡ˆ**ï¼šä½æ–¼ `../translations/{book_info['id']}/` ç›®éŒ„
3. **ç¿»è­¯æ¨¡æ¿**ï¼šå·²è‡ªå‹•ç”Ÿæˆï¼Œå¯ç›´æ¥ç·¨è¼¯

## ç¿»è­¯é€²åº¦

- [x] åŸæ–‡çˆ¬å–å®Œæˆ
- [x] ç¿»è­¯æ¨¡æ¿ç”Ÿæˆå®Œæˆ
- [ ] äººå·¥ç¿»è­¯å¾…å®Œæˆ

---
*å°ˆæ¡ˆå»ºç«‹æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ä½¿ç”¨å·¥å…·ï¼šå…¨è‡ªå‹•å¤ç±ç¿»è­¯ç³»çµ±*
"""
        
        readme_path = self.project_root / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
            
        # è¿½è¹¤READMEæª”æ¡ˆ
        track_file_write(readme_path, "project_readme")
        log_operation("create", readme_path, {
            "book_id": book_info['id'],
            "book_title": book_info['title'],
            "chapter_count": len(chapters)
        })
            
        print(f"âœ… å·²å»ºç«‹å°ˆæ¡ˆèªªæ˜: {readme_path}")
        
    def run_full_automation(self):
        """åŸ·è¡Œå®Œæ•´çš„è‡ªå‹•åŒ–æµç¨‹"""
        print("ğŸš€ å•Ÿå‹•å…¨è‡ªå‹•å¤ç±ç¿»è­¯ç³»çµ±")
        print("=" * 50)
        
        # 1. ç²å–æ›¸ç±è³‡è¨Š
        book_info = self.get_book_info()
        print(f"ğŸ“š æ›¸ç±ï¼š{book_info['title']}")
        print(f"ğŸ‘¤ ä½œè€…ï¼š{book_info['author']}")
        
        # 2. è¨­å®šå°ˆæ¡ˆçµæ§‹ï¼ˆä½¿ç”¨æ›¸åï¼‰
        self.setup_project_structure(book_info)
        print(f"ğŸ“ å°ˆæ¡ˆè³‡æ–™å¤¾ï¼š{self.folder_name}")
        
        # 3. ç²å–ç« ç¯€åˆ—è¡¨
        chapters = self.get_chapter_list()
        if not chapters:
            print("âŒ ç„¡æ³•ç²å–ç« ç¯€åˆ—è¡¨ï¼Œç¨‹åºçµ‚æ­¢")
            return False
            
        print(f"ğŸ“‹ æ‰¾åˆ° {len(chapters)} å€‹ç« ç¯€")
        
        # 4. æ‰¹é‡çˆ¬å–å’Œç¿»è­¯
        success_count = 0
        for chapter in chapters:
            print(f"\nğŸ”„ è™•ç†ç¬¬ {chapter['number']} ç« ...")
            
            # çˆ¬å–åŸæ–‡
            content_data = self.crawl_chapter(chapter)
            if content_data:
                # ç”Ÿæˆç¿»è­¯æ¨¡æ¿
                self.translate_chapter(content_data, chapter['number'])
                success_count += 1
                
            # æ·»åŠ å»¶é²é¿å…è¢«å°é–
            time.sleep(2)
            
        # 5. å»ºç«‹å°ˆæ¡ˆæ–‡æª”
        self.create_project_readme(book_info, chapters)
        
        # 6. è¿½è¹¤æ–°ç¶“å…¸åˆ°ç³»çµ±
        if success_count > 0:
            print("\nğŸ“Š æ›´æ–°ç¶“å…¸è¿½è¹¤ç³»çµ±...")
            try:
                # æº–å‚™ç« ç¯€è³‡è¨Š
                processed_chapters = []
                for i, chapter in enumerate(chapters[:success_count], 1):
                    processed_chapters.append({
                        'number': i,
                        'title': chapter.get('title', f'ç¬¬{i}ç« '),
                        'url': chapter.get('url', '')
                    })
                
                # è¿½è¹¤æ–°ç¶“å…¸
                track_new_classic(
                    book_info=book_info,
                    chapters=processed_chapters,
                    source_dir=self.project_root,
                    translation_dir=self.translation_dir
                )
                
                # ç”Ÿæˆæœ€æ–°çš„è¿½è¹¤å ±å‘Š
                generate_tracking_report()
                print("âœ… ç¶“å…¸è¿½è¹¤ç³»çµ±å·²æ›´æ–°")
                
            except Exception as e:
                print(f"âš ï¸  è¿½è¹¤ç³»çµ±æ›´æ–°å¤±æ•—: {e}")
        
        # 7. ç”Ÿæˆç¸½çµå ±å‘Š
        print(f"\nğŸ‰ è‡ªå‹•åŒ–å®Œæˆï¼")
        print(f"âœ… æˆåŠŸè™•ç†ï¼š{success_count}/{len(chapters)} ç« ")
        print(f"ğŸ“ å°ˆæ¡ˆä½ç½®ï¼š{self.project_root}")
        print(f"ğŸ“ ç¿»è­¯æª”æ¡ˆï¼š{self.translation_dir}")
        print(f"ğŸ“Š è¿½è¹¤è¨˜éŒ„ï¼šç¶“å…¸è¿½è¹¤è¨˜éŒ„.json")
        print(f"ğŸ“‹ æœ€æ–°å ±å‘Šï¼šç¶“å…¸è¿½è¹¤å ±å‘Š.md")
        
        return success_count > 0

def main():
    """ä¸»å‡½æ•¸ - æ”¯æ´å¤šæ›¸ç±è™•ç†"""
    
    # æ›¸ç±URLåˆ—è¡¨ - æ‚¨å¯ä»¥åœ¨é€™è£¡æ·»åŠ å¤šæœ¬æ›¸
    book_urls = [
        "https://www.shidianguji.com/book/SBCK109",  # æŠ±æœ´å­
        # "https://www.shidianguji.com/book/SBCK001",  # å…¶ä»–æ›¸ç±
        # å¯ä»¥æ·»åŠ æ›´å¤šæ›¸ç±URL
    ]
    
    print("ğŸŒŸ å…¨è‡ªå‹•å¤ç±ç¿»è­¯ç³»çµ±")
    print("=" * 60)
    
    for i, book_url in enumerate(book_urls, 1):
        print(f"\nğŸ“– è™•ç†ç¬¬ {i} æœ¬æ›¸ç±: {book_url}")
        print("-" * 40)
        
        try:
            translator = AutoTranslator(book_url)
            success = translator.run_full_automation()
            
            if success:
                print(f"âœ… ç¬¬ {i} æœ¬æ›¸è™•ç†å®Œæˆ")
            else:
                print(f"âŒ ç¬¬ {i} æœ¬æ›¸è™•ç†å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ è™•ç†ç¬¬ {i} æœ¬æ›¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
        print("-" * 40)
        
    print(f"\nğŸŠ æ‰€æœ‰æ›¸ç±è™•ç†å®Œæˆï¼")
    print("ğŸ’¡ æç¤ºï¼šç¿»è­¯æ¨¡æ¿å·²ç”Ÿæˆï¼Œæ‚¨å¯ä»¥ç›´æ¥ç·¨è¼¯ç¿»è­¯å…§å®¹")

if __name__ == "__main__":
    main()